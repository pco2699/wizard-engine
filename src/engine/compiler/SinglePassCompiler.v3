// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

var config_TrackFloatZeroConst = false;
def INITIAL = 16;
def OUT = Trace.OUT;
def regRefCounts = Array<int>.new(128);
def TMP_SLOT = 1000000000;

// Expose constants outside this file.
component SpcConsts {
	// Abstract values tracked during single-pass compilation.
	def NO_REG = Reg(0);
	def IS_STORED: byte = 0x01;
	def IS_CONST: byte = 0x02;
	def IN_REG: byte = 0x04;
	def TAG_STORED: byte = 0x08;
	def KIND_MASK: byte = 0xF0;
	def KIND_I32: byte = kindToFlags(ValueKind.I32);
	def KIND_I64: byte = kindToFlags(ValueKind.I64);
	def KIND_F32: byte = kindToFlags(ValueKind.F32);
	def KIND_F64: byte = kindToFlags(ValueKind.F64);
	def KIND_V128: byte = kindToFlags(ValueKind.V128);
	def KIND_REF: byte = kindToFlags(ValueKind.REF);
	def KIND_ABS: byte = kindToFlags(ValueKind.ABS);
	def kinds: Array<ValueKind> = [ValueKind.I32, ValueKind.I64, ValueKind.F32, ValueKind.F64,
					ValueKind.V128, ValueKind.REF, ValueKind.ABS];
	def kindToFlags(kind: ValueKind) -> byte {
		return byte.view(kind.tag) << 4;
	}
}

// Shorten constants inside this file.
def NO_REG = SpcConsts.NO_REG;
def IS_STORED = SpcConsts.IS_STORED;
def IS_CONST = SpcConsts.IS_CONST;
def IN_REG = SpcConsts.IN_REG;
def TAG_STORED = SpcConsts.TAG_STORED;
def KIND_MASK = SpcConsts.KIND_MASK;
def KIND_I32 = SpcConsts.KIND_I32;
def KIND_I64 = SpcConsts.KIND_I64;
def KIND_F32 = SpcConsts.KIND_F32;
def KIND_F64 = SpcConsts.KIND_F64;
def KIND_V128 = SpcConsts.KIND_V128;
def KIND_REF = SpcConsts.KIND_REF;
def KIND_ABS = SpcConsts.KIND_ABS;

// Compiles Wasm bytecode to machine code in a single pass via a MacroAssembler.
class SinglePassCompiler(masm: MacroAssembler, extensions: Extension.set, limits: Limits, err: ErrorGen) extends BytecodeVisitor {
	def it = BytecodeIterator.new();
	def instrTracer = if(Trace.compiler, InstrTracer.new());
	def config = masm.regAlloc.regConfig;
	def regs = config.regs;
	def resolver = SpcMoveResolver.new(masm);
	def unrefs = Array<(Reg, int)>.new(config.regSet.regs.length);
	var num_unrefs = 0;

	// Abstract state of the value stack
	def state = SpcState.new(masm.regAlloc);
	// Other state
	var start_pos = 0;
	var module: Module;
	var func: FuncDecl;
	var sig: SigDecl;
	var success = true;
	var ret_label: MasmLabel;
	var unwind_label: MasmLabel;
	var last_probe = 0;
	var skip_to_end: bool;

	new() {
		masm.unimplemented = unsupported;
		masm.scratchStackSlot1 = config.eip_offset;
		masm.scratchStackSlot2 = config.stp_offset;
	}

	def gen(module: Module, func: FuncDecl) -> bool {
		if (Trace.compiler) OUT.put1("==== begin compile: %q ========================", func.render(module.names, _)).outln();
		// Metric collection: only call System.ticksUs() if time metric enabled.
		var time_metric = Metrics.spc_time_us;
		var before_us = if(time_metric.enabled, System.ticksUs());
		var before_code_bytes = masm.curCodeBytes();
		var before_data_bytes = masm.curDataBytes();

		// Reset internal state.
		this.module = module;
		this.func = func;
		it.reset(func);
		sig = func.sig;
		unwind_label = null;
		masm.regAlloc.clear();
		success = true;

		// Initialize parameters, locals, and first control stack entry.
		var end_label = masm.newLabel(func.cur_bytecode.length);
		state.reset(sig, end_label);

		// Visit all local declarations.
		it.dispatchLocalDecls(this);

		// Emit prologue, which allocates the frame and initializes various registers.
		emitPrologue();

		// Emit instructions.
		while (it.more() && success) {
			if (Trace.compiler) traceOpcodeAndStack();
			last_probe = 0;
			it.dispatch(this);
			unrefRegs();
			if (Debug.compiler) checkRegAlloc();
			it.next();
			if (skip_to_end) doSkipToEndOfBlock();
		}

		// Emit epilogue even if no return.
		if (unwind_label != null) {
			masm.bindLabel(unwind_label);
			unwind_label = null;
			emitUnwind();
		}
		if (success) {
			// Metric collection
			if (time_metric.enabled) time_metric.val += u32.view(System.ticksUs() - before_us);
			Metrics.spc_in_bytes.val += u32.view(func.orig_bytecode.length);
			Metrics.spc_code_bytes.val += (masm.curCodeBytes() - before_code_bytes);
			Metrics.spc_data_bytes.val += (masm.curDataBytes() - before_data_bytes);
			Metrics.spc_functions.val++;
		}
		return success;
	}
	def doSkipToEndOfBlock() {
		skip_to_end = false;
		var height = 0;
		while (it.more() && success) {
			var opcode = it.current();
			match (opcode) {
				BLOCK, LOOP, TRY, IF => height++;
				ELSE, CATCH, CATCH_ALL => if (height == 0) return;
				END, DELEGATE => if (height-- == 0) return;
				_ => ;
			}
			if (Trace.compiler) traceOpcodeUnreachable();
			it.next();
		}
	}
	def checkRegAlloc() {
		masm.regAlloc.debugTODO();
		if (Trace.compiler) {
			OUT.puts("checkRegAlloc ");
			state.trace();
		}
		for (i < regRefCounts.length) regRefCounts[i] = 0;
		for (i < state.sp) {
			var sv = state.state[i];
			if (sv.inReg())  {
				var ri = sv.reg.index;
				assert1(ri != 0, "state[%d].inReg but reg == 0", i);
				assert1(ri < config.regSet.length, "state[%d], has invalid register", i);
				var pool = config.regToPool[ri];
				assert1(pool >= 0 && pool < config.numRegPools, "state[%d] has invalid register pool", i);
				var poolk = config.kindToPool[sv.kind().tag];
				assert1(pool == poolk, "state[%d] differs on pool membership", i);
				regRefCounts[ri]++;
//TODO				var slot = masm.regAlloc.get(sv.reg);
//TODO				assert3(slot == i, "state[%d].reg = %s, but regalloc = %d", i, config.regSet.getName(sv.reg), slot);
			}
		}
		for (i = 1; i < config.regSet.length; i++) {
			var r = Reg(byte.view(i));
//TODO			var slot = masm.regAlloc.get(r);
			var slot: int;
			if (regRefCounts[i] == 0) {
				assert2(masm.regAlloc.isFree(r), "%s should be free, got slot = %d", config.regSet.getName(r), slot);
			} else {
				assert2(!masm.regAlloc.isFree(r), "%s should be allocated, got slot = %d", config.regSet.getName(r), slot);
			}
		}
	}
	def assert1<T>(cond: bool, msg: string, param: T) {
		if (!cond) bailout(Strings.format1(msg, param));
	}
	def assert2<T, U>(cond: bool, msg: string, p1: T, p2: U) {
		if (!cond) bailout(Strings.format2(msg, p1, p2));
	}
	def assert3<T, U, V>(cond: bool, msg: string, p1: T, p2: U, p3: V) {
		if (!cond) bailout(Strings.format3(msg, p1, p2, p3));
	}
	def emitPrologue() {
		var offsets = masm.getOffsets();
		// Allocate stack frame
		masm.emit_subw_r_i(regs.sp, config.spcFrameSize);

		// Spill VSP
		masm.emit_mov_m_r(ValueKind.REF, MasmAddr(regs.sp, config.vsp_offset), regs.vsp); // XXX: track VSP-spilled state
		// Spill wf: WasmFunction
		masm.emit_mov_m_r(ValueKind.REF, MasmAddr(regs.sp, config.wasm_func_offset), regs.func_arg);
		// Load wf.instance and spill
		masm.emit_mov_r_m(ValueKind.REF, regs.instance, MasmAddr(regs.func_arg, offsets.WasmFunction_instance));
		masm.emit_mov_m_r(ValueKind.REF, MasmAddr(regs.sp, config.instance_offset), regs.instance);
		// Clear FrameAccessor
		masm.emit_mov_m_l(MasmAddr(regs.sp, config.accessor_offset), 0); // XXX: value kind

		// Compute VFP = VSP - sig.params.length * SLOT_SIZE
		masm.emit_mov_r_r(regs.vfp, regs.vsp); // XXX: use 3-addr adjustment of VFP
		masm.emit_subw_r_i(regs.vfp, sig.params.length * masm.valuerep.slot_size);
		// XXX: skip spilling of VFP
		masm.emit_mov_m_r(ValueKind.REF, MasmAddr(regs.sp, config.vfp_offset), regs.vfp);

		// Load instance.memories[0].start into MEM0_BASE and spill
		if (module.memories.length > 0) {
			// XXX: skip loading memory base if function doesn't access memory
			masm.emit_mov_r_m(ValueKind.REF, regs.mem0, MasmAddr(regs.instance, offsets.Instance_memories));
			masm.emit_read_v3_array_r_i(ValueKind.REF, regs.mem0, regs.mem0, 0);
			masm.emit_read_v3_mem_base(regs.mem0, regs.mem0);
			masm.emit_mov_m_r(ValueKind.REF, MasmAddr(regs.sp, config.mem_offset), regs.mem0);
		}
	}
	def visitLocalDecl(count: u32, vtc: ValueTypeCode) {
		state.addLocals(count, vtc.toAbstractValueType(module));
	}
	def visitOp(opcode: Opcode) {
		bailout(Strings.format1("unsupported opcode: %s", opcode.name));
	}

	def visitProbe() {
		var b = func.orig_bytecode[it.pc];
		last_probe = it.pc;
		if (b != Opcode.LOOP.code && b != Opcode.END.code && b != Opcode.ELSE.code) emitProbe();
	}
	def emitProbe() {
		if (last_probe == 0) return;
		last_probe = 0;
		// spill everything
		state.emitKill(resolver);
		// compute VSP for potential frame access
		emit_compute_vsp(regs.vsp, state.sp);
		masm.emit_mov_m_r(ValueKind.REF, MasmAddr(regs.sp, config.vsp_offset), regs.vsp);
		masm.emit_store_runtime_vsp(regs.vsp);
		// reload WasmFunction
		masm.emit_mov_r_m(ValueKind.REF, regs.runtime_arg_0, MasmAddr(regs.sp, config.wasm_func_offset));
		// load PC
		masm.emit_mov_r_i(regs.runtime_arg_1, it.pc);
		// call runtime
		masm.emit_call_runtime_Probe_instr();
		emit_reload_regs();
	}
	def visit_CRASH_EXEC() {
		masm.emit_intentional_crash();
	}
	def visit_CRASH_COMPILER() {
		System.error("WizengError", "encountered crash-compiler opcode");
	}
	def visit_UNREACHABLE() {
		emitTrap(TrapReason.UNREACHABLE);
		setUnreachable();
	}
	def visit_NOP() {
		// emit nothing
	}
	def visit_BLOCK(btc: BlockTypeCode) {
		var pr = btc.toAbstractBlockType(module);
		state.pushBlock(pr.0, pr.1, masm.newLabel(it.pc));
	}
	def visit_LOOP(btc: BlockTypeCode) {
		var pr = btc.toAbstractBlockType(module);
		state.pushLoop(pr.0, pr.1, masm.newLabel(it.pc));
		var ctl_top = state.ctl_stack.peek();
		state.prepareLoop(resolver);
		masm.bindLabel(ctl_top.label);
		emitProbe();
	}
	def visit_IF(btc: BlockTypeCode) {
		var pr = btc.toAbstractBlockType(module);
		var sv = pop();
		var ctl_top = state.pushIf(pr.0, pr.1, masm.newLabel(it.pc), masm.newLabel(it.pc));
		emitBrIf(sv, MasmBrCond.I32_ZERO, ctl_top.else_label, ctl_top, true);
	}
	def visit_ELSE() {
		var ctl_top = state.ctl_stack.peek();
		state.emitFallthru(resolver);
		masm.emit_br(ctl_top.label);
		masm.bindLabel(ctl_top.else_label);
		state.doElse();
		ctl_top.opcode = Opcode.ELSE.code;
		emitProbe();
	}
	def visit_END() {
		var ctl_top = state.ctl_stack.peek();
		if (ctl_top.opcode == Opcode.LOOP.code) {
			state.ctl_stack.pop();
			if (!ctl_top.reachable) setUnreachable();
		} else if (ctl_top.opcode == Opcode.IF.code) {
			// simulate empty if-true block
			state.emitFallthru(resolver);
			masm.emit_br(ctl_top.label);
			masm.bindLabel(ctl_top.else_label);
			state.doElse();
			ctl_top.opcode = Opcode.ELSE.code;
			state.emitFallthru(resolver);
			masm.bindLabel(ctl_top.label);
			state.resetToMerge(ctl_top);
			state.ctl_stack.pop();
		} else if (ctl_top.opcode == Opcode.BLOCK.code || ctl_top.opcode == Opcode.ELSE.code) {
			state.emitFallthru(resolver);
			masm.bindLabel(ctl_top.label);
			state.resetToMerge(ctl_top);
			state.ctl_stack.pop();
		} else if (ctl_top.opcode == Opcode.RETURN.code) {
			state.emitFallthru(resolver);
			masm.bindLabel(ctl_top.label);
			emitProbe();
			if (ctl_top.merge_count > 1) emitReturn(ctl_top);
			state.ctl_stack.pop();
		}
		emitProbe();
	}
	def visit_BR(depth: u31) {
		var target = state.getControl(depth);
		state.emitTransfer(target, resolver);
		masm.emit_br(target.label);
		setUnreachable();
	}
	def visit_BR_IF(depth: u31) {
		var target = state.getControl(depth);
		var sv = pop();
		emitBrIf(sv, MasmBrCond.I32_NONZERO, target.label, target, state.isTransferEmpty(target));
	}
	def visit_BR_ON_NULL(depth: u31) {
		var target = state.getControl(depth);
		var sv = pop();
		emitBrIf(sv, MasmBrCond.REF_NULL, target.label, target, state.isTransferEmpty(target));
	}
	def visit_BR_ON_NON_NULL(depth: u31) {
		var target = state.getControl(depth);
		var sv = pop();
		emitBrIf(sv, MasmBrCond.REF_NONNULL, target.label, target, state.isTransferEmpty(target));
	}
	def visit_BR_TABLE(depths: Range<u31>) {
		var sv = pop();
		emitBrTable(sv, depths);
		setUnreachable();
	}
	def visit_RETURN() {
		var target = state.ctl_stack.elems[0];
		state.emitTransfer(target, resolver);
		if (ret_label == null) ret_label = masm.newLabel(func.cur_bytecode.length);
		masm.emit_br(ret_label);
		setUnreachable();
	}
	def visitCallDirect(op: Opcode, index: u31, tailCall: bool) {
		var func = module.functions[index];
		var offsets = masm.getOffsets();
		var retpt = masm.newLabel(it.pc), wasmcall_label = masm.newLabel(it.pc);
		// Load the instance (which must happen before frame is unwound).
		var vsp_reg = allocTmpFixed(ValueKind.REF, regs.vsp);
		var func_reg = allocTmpFixed(ValueKind.REF, regs.func_arg);
		var tmp = allocTmp(ValueKind.REF);
		emit_load_instance(tmp);

		// Handle the current stack state.
		if (tailCall) emitMoveTailCallArgs(func.sig); // transfer tail call args
		else state.emitKill(resolver); // spill entire value stack

		// Load the function, XXX: skip and compute function from instance + code on stack?
		masm.emit_mov_r_m(ValueKind.REF, func_reg, MasmAddr(tmp, offsets.Instance_functions));
		masm.emit_read_v3_array_r_i(ValueKind.REF, func_reg, func_reg, func.func_index);

		// Compute the value stack pointer.
		emit_compute_vsp(vsp_reg, state.sp);

		if (func.imp != null) {
			// A call to imported function must first check for WasmFunction.
			masm.emit_mov_r_m(ValueKind.I32, tmp, MasmAddr(func_reg, 0));
			masm.emit_breq_r_i(tmp, offsets.WasmFunction_typeId, wasmcall_label);
			masm.emit_store_runtime_vsp(regs.vsp);
			masm.emit_mov_r_r(regs.runtime_arg_0, func_reg);
			if (tailCall) {
				masm.emit_jump_runtime_callHost(func_reg);
			} else {
				masm.emit_call_runtime_callHost(func_reg);
				masm.emit_br(retpt);
			}
		}
		// Load the target code/entrypoint.
		masm.bindLabel(wasmcall_label);
		masm.emit_mov_r_m(ValueKind.REF, tmp, MasmAddr(func_reg, offsets.WasmFunction_decl));
		masm.emit_mov_r_m(ValueKind.REF, tmp, MasmAddr(tmp, offsets.FuncDecl_target_code));

		// Call or jump to the entrypoint.
		if (tailCall) {
			masm.emit_jump_r(tmp);
			setUnreachable();
		} else {
			masm.emit_call_r(tmp);
			masm.bindLabel(retpt);
			emit_unwind_check();
			emit_reload_regs();
			state.popArgsAndPushResults(func.sig);
		}
	}
	def emitMoveTailCallArgs(sig: SigDecl) {
		var p = sig.params, count = u32.!(p.length);
		var base = state.sp - count;
		for (i < count) { // transfer args
			var tv = SpcVal(typeToKindFlags(p[i]) | IS_STORED | TAG_STORED, NO_REG, 0); // XXX: skip tag copy
			var fv = state.state[base + i];
			resolver.addMove((i, tv), (base + i, fv));
		}
		for (i < base) unrefSlot(i); // free all unused slots of frame below args
		resolver.emitMoves();
		state.sp = count;
		// adjust frame
		masm.emit_addw_r_i(regs.sp, config.spcFrameSize);
	}
	def visitCallIndirect(op: Opcode, sig_index: u31, table_index: u31, tailCall: bool) {
		var sig = SigDecl.!(module.heaptypes[sig_index]);
		var offsets = masm.getOffsets();
		var retpt = masm.newLabel(it.pc), wasmcall_label = masm.newLabel(it.pc);
		var vsp_reg = allocTmpFixed(ValueKind.REF, regs.vsp);
		var sv = popFixedReg(regs.func_arg);
		var func_reg = sv.reg;
		emit_load_instance(regs.instance);

		var treg = allocTmp(ValueKind.REF);
		var tmp2 = allocTmp(ValueKind.REF);

		// tmp1 = load table[table_index] from instance
		masm.emit_mov_r_m(ValueKind.REF, treg, MasmAddr(regs.instance, offsets.Instance_tables));
		masm.emit_read_v3_array_r_i(ValueKind.REF, treg, treg, int.!(table_index));
		// bounds check
		masm.emit_mov_r_m(ValueKind.REF, tmp2, MasmAddr(treg, offsets.Table_ids));
		masm.emit_bounds_check_v3_array(tmp2, func_reg, masm.newTrapLabel(TrapReason.FUNC_INVALID));
		// signature check
		masm.emit_read_v3_array_r_r(ValueKind.I32, tmp2, tmp2, func_reg); // XXX: using ref reg as int
		masm.emit_breq_r_i(tmp2, -1, masm.newTrapLabel(TrapReason.FUNC_INVALID));
		masm.emit_brne_r_i(tmp2, sig.canon_id, masm.newTrapLabel(TrapReason.FUNC_SIG_MISMATCH));
		// load from table
		masm.emit_mov_r_m(ValueKind.REF, tmp2, MasmAddr(treg, offsets.Table_funcs));
		masm.emit_read_v3_array_r_r(ValueKind.REF, func_reg, tmp2, func_reg);

		// Handle the current stack state.
		if (tailCall) emitMoveTailCallArgs(sig); // transfer tail call args
		else state.emitKill(resolver); // spill entire value stack

		emit_compute_vsp(vsp_reg, state.sp);

		// check for WasmFunction
		masm.emit_mov_r_m(ValueKind.I32, tmp2, MasmAddr(func_reg, 0));
		masm.emit_breq_r_i(tmp2, offsets.WasmFunction_typeId, wasmcall_label);

		masm.emit_store_runtime_vsp(vsp_reg);
		masm.emit_mov_r_r(regs.runtime_arg_0, func_reg);
		if (tailCall) {
			masm.emit_jump_runtime_callHost(func_reg);
		} else {
			masm.emit_call_runtime_callHost(func_reg);
			masm.emit_br(retpt);
		}

		masm.bindLabel(wasmcall_label);
		masm.emit_mov_r_m(ValueKind.REF, tmp2, MasmAddr(func_reg, offsets.WasmFunction_decl));
		masm.emit_mov_r_m(ValueKind.REF, tmp2, MasmAddr(tmp2, offsets.FuncDecl_target_code));

		// Call or jump to the entrypoint.
		if (tailCall) {
			masm.emit_jump_r(tmp2);
			setUnreachable();
		} else {
			masm.emit_call_r(tmp2);
			masm.bindLabel(retpt);
			emit_unwind_check();
			emit_reload_regs();
			state.popArgsAndPushResults(sig);
		}
	}
	def visit_CALL_REF(index: u31) {
		var sig = SigDecl.!(module.heaptypes[index]);
		var offsets = masm.getOffsets();
		var vsp_reg = allocTmpFixed(ValueKind.REF, regs.vsp);
		var sv = state.peek();
		if (sv.isConst() && sv.const == 0) {
			emitTrap(TrapReason.NULL_DEREF);
			setUnreachable();
			return;
		}
		sv = popFixedReg(regs.func_arg);
		var func_reg = sv.reg;
		var retpt = masm.newLabel(it.pc), wasmcall_label = masm.newLabel(it.pc);
		state.emitKill(resolver);

		emit_compute_vsp(vsp_reg, state.sp);
		// check for null
		masm.emit_breq_r_l(func_reg, 0, masm.newTrapLabel(TrapReason.NULL_DEREF));

		var tmp = allocTmp(ValueKind.REF);
		masm.emit_mov_r_m(ValueKind.I32, tmp, MasmAddr(func_reg, 0));
		masm.emit_breq_r_i(tmp, offsets.WasmFunction_typeId, wasmcall_label);
		// not a WasmFunction
		masm.emit_store_runtime_vsp(regs.vsp);
		masm.emit_mov_r_r(regs.runtime_arg_0, func_reg);
		masm.emit_call_runtime_callHost(func_reg);
		masm.emit_br(retpt);

		// WasmFunction
		masm.bindLabel(wasmcall_label);
		masm.emit_mov_r_m(ValueKind.REF, tmp, MasmAddr(func_reg, offsets.WasmFunction_decl));
		masm.emit_mov_r_m(ValueKind.REF, tmp, MasmAddr(tmp, offsets.FuncDecl_target_code));
		masm.emit_call_r(tmp);

		masm.bindLabel(retpt);
		emit_unwind_check();
		emit_reload_regs();

		state.popArgsAndPushResults(sig);
	}
	def visit_DROP() {
		drop();
	}
	def visit_SELECT() {
		emitSelect(1);
	}
	def visit_SELECT_T(val_types: Range<ValueTypeCode>) {
		emitSelect(u32.!(val_types.length));
	}
	def emitSelect(count: u32) {
		var sv = pop();
		var base: u32 = state.sp - count*2;
		if (sv.isConst()) {
			// XXX: select K* K* v => K* if all constants match
			if (sv.const != 0) {
				// select a* b* T => a*
				for (i < count) drop();
			} else {
				// select a* b* 0 => b*
				for (i < count) {
					var ti = base + i;
					var fi = base + i + count;
					var fv = state.state[fi];
					unrefSlot(ti);
					if (fv.inReg()) reassignReg(fv.reg, int.!(ti));
					var tv = state.state[ti];
					state.state[ti] = SpcVal(tv.kindFlagsAndTag(fv.flags & (IN_REG | IS_CONST)), fv.reg, fv.const);
				}
			}
			return;
		}
		state.doSelectTrueTransferInPlace(count, resolver);
		var label = masm.newLabel(it.pc);
		var cond = MasmBrCond.I32_NONZERO;
		if (sv.inReg()) masm.emit_br_r(sv.reg, cond, label);
		else masm.emit_br_m(masm.slotAddr(state.sp), cond, label);
		state.doSelectFalseTransferInPlace(count, resolver);
		resolver.emitMoves();
		masm.bindLabel(label);
		state.sp = base + count;
	}
	def visit_LOCAL_GET(index: u31) {
		var lv = state.get(index);
		if (lv.inReg()) {
			masm.regAlloc.assign(lv.reg, int.!(state.sp));
			var isConst = lv.flags & IS_CONST;
			state.push(lv.kindFlags(isConst | IN_REG), lv.reg, lv.const);
		} else if (lv.isConst()) {
			state.push(lv.kindFlags(IS_CONST), NO_REG, lv.const);
		} else {
			var kind = lv.kind();
			var reg = allocRegTos(kind);
			masm.emit_mov_r_s(kind, reg, index);
			state.push(lv.kindFlags(IN_REG), reg, 0);
			state.state[index] = SpcVal(lv.flags | IN_REG, reg, 0);
			masm.regAlloc.assign(reg, index);
		}
	}
	def visit_LOCAL_SET(index: u31) {
		var lv = state.get(index);
		var sv = state.pop();
		if (sv.inReg()) {
			masm.regAlloc.unassign(lv.reg, index); // unref existing register
			var isConst = sv.flags & IS_CONST;
			state.set(index, lv.kindFlagsAndTag(IN_REG | isConst), sv.reg, sv.const);
			masm.regAlloc.reassign(sv.reg, int.!(state.sp), index);
		} else if (sv.isConst()) {
			masm.regAlloc.unassign(lv.reg, index); // unref existing register
			state.set(index, lv.kindFlagsAndTag(IS_CONST), NO_REG, sv.const);
		} else {
			var kind = lv.kind();
			var reg: Reg;
			if (masm.regAlloc.frequency(lv.reg) == 1) {
				reg = lv.reg; // used only once; can reuse
			} else {
				masm.regAlloc.unassign(lv.reg, index); // unref existing register
				reg = allocReg(kind, index);           // and allocate a new one
			}
			masm.emit_mov_r_s(kind, reg, state.sp);
			state.set(index, lv.kindFlagsAndTag(IN_REG), reg, 0);
		}
	}
	def visit_LOCAL_TEE(index: u31) {
		var lv = state.get(index);
		masm.regAlloc.unassign(lv.reg, index); // unref existing register
		var sv = state.peek();
		if (sv.inReg()) {
			masm.regAlloc.assign(sv.reg, index);
			var isConst = sv.flags & IS_CONST;
			state.set(index, lv.kindFlagsAndTag(IN_REG | isConst), sv.reg, sv.const);
		} else if (sv.isConst()) {
			state.set(index, lv.kindFlagsAndTag(IS_CONST), NO_REG, sv.const);
		} else {
			var tos = state.sp - 1;
			var kind = lv.kind();
			var reg = allocReg(kind, tos);
			masm.regAlloc.assign(reg, index);
			masm.emit_mov_r_s(kind, reg, tos);
			state.set(index, lv.kindFlagsAndTag(IN_REG), reg, 0);
			state.overwrite(lv.kindFlagsAndTag(IN_REG | IS_STORED), reg, 0);
		}
	}
	def visit_GLOBAL_GET(index: u31) {
		var global = module.globals[index];
		if (!global.mutable && global.imp == null && InitExpr.I32.?(global.init)) {
			state.push(KIND_I32 | IS_CONST, NO_REG, InitExpr.I32.!(global.init).val);
		} else {
			emit_call_runtime_op1n(Opcode.GLOBAL_GET, index, 0, [global.valtype], false);
		}
	}
	def visit_GLOBAL_SET(index: u31) {
		emit_call_runtime_op1n(Opcode.GLOBAL_SET, index, 1, ValueTypes.NONE, false);
	}
	def visit_TABLE_GET(index: u31) {
		var table = module.tables[index];
		emit_call_runtime_op1n(Opcode.TABLE_GET, index, 1, [table.elemtype], true);
	}
	def visit_TABLE_SET(index: u31) {
		var table = module.tables[index];
		emit_call_runtime_op1n(Opcode.TABLE_SET, index, 2, ValueTypes.NONE, true);
	}

	def visit_I32_LOAD(imm: MemArg) { emitLoad(ValueKind.I32, imm, masm.emit_load_r_r_r_i); }
	def visit_I64_LOAD(imm: MemArg) { emitLoad(ValueKind.I64, imm, masm.emit_load_r_r_r_i); }
	def visit_F32_LOAD(imm: MemArg) { emitLoad(ValueKind.F32, imm, masm.emit_load_r_r_r_i); }
	def visit_F64_LOAD(imm: MemArg) { emitLoad(ValueKind.F64, imm, masm.emit_load_r_r_r_i); }
	def visit_I32_LOAD8_S(imm: MemArg) { emitLoad(ValueKind.I32, imm, masm.emit_loadbsx_r_r_r_i); }
	def visit_I32_LOAD8_U(imm: MemArg) { emitLoad(ValueKind.I32, imm, masm.emit_loadbzx_r_r_r_i); }
	def visit_I32_LOAD16_S(imm: MemArg) { emitLoad(ValueKind.I32, imm, masm.emit_loadwsx_r_r_r_i); }
	def visit_I32_LOAD16_U(imm: MemArg) { emitLoad(ValueKind.I32, imm, masm.emit_loadwzx_r_r_r_i); }
	def visit_I64_LOAD8_S(imm: MemArg) { emitLoad(ValueKind.I64, imm, masm.emit_loadbsx_r_r_r_i); }
	def visit_I64_LOAD8_U(imm: MemArg) { emitLoad(ValueKind.I64, imm, masm.emit_loadbzx_r_r_r_i); }
	def visit_I64_LOAD16_S(imm: MemArg) { emitLoad(ValueKind.I64, imm, masm.emit_loadwsx_r_r_r_i); }
	def visit_I64_LOAD16_U(imm: MemArg) { emitLoad(ValueKind.I64, imm, masm.emit_loadwzx_r_r_r_i); }
	def visit_I64_LOAD32_S(imm: MemArg) { emitLoad(ValueKind.I64, imm, masm.emit_loaddsx_r_r_r_i); }
	def visit_I64_LOAD32_U(imm: MemArg) { emitLoad(ValueKind.I64, imm, masm.emit_loaddzx_r_r_r_i); }
	def visit_I32_STORE(imm: MemArg) { emitStore(ValueKind.I32, imm, masm.emit_store_r_r_r_i); }
	def visit_I64_STORE(imm: MemArg) { emitStore(ValueKind.I64, imm, masm.emit_store_r_r_r_i); }
	def visit_F32_STORE(imm: MemArg) { emitStore(ValueKind.F32, imm, masm.emit_store_r_r_r_i); }
	def visit_F64_STORE(imm: MemArg) { emitStore(ValueKind.F64, imm, masm.emit_store_r_r_r_i); }
	def visit_I32_STORE8(imm: MemArg) { emitStore(ValueKind.I32, imm, masm.emit_storeb_r_r_r_i); }
	def visit_I32_STORE16(imm: MemArg) { emitStore(ValueKind.I32, imm, masm.emit_storew_r_r_r_i); }
	def visit_I64_STORE8(imm: MemArg) { emitStore(ValueKind.I64, imm, masm.emit_storeb_r_r_r_i); }
	def visit_I64_STORE16(imm: MemArg) { emitStore(ValueKind.I64, imm, masm.emit_storew_r_r_r_i); }
	def visit_I64_STORE32(imm: MemArg) { emitStore(ValueKind.I32, imm, masm.emit_store_r_r_r_i); }

	def visit_MEMORY_SIZE(memory_index: u31) {
		var offsets = masm.getOffsets();
		var reg = allocTmp(ValueKind.REF);
		var r1 = allocRegTos(ValueKind.I64);
		var r2 = allocTmp(ValueKind.I64);
		emit_load_instance(reg);
		masm.emit_mov_r_m(ValueKind.REF, reg, MasmAddr(reg, offsets.Instance_memories));
		masm.emit_read_v3_array_r_i(ValueKind.REF, reg, reg, int.!(memory_index));
		masm.emit_mov_r_m(ValueKind.REF, r2, MasmAddr(reg, offsets.X86_64Memory_start));
		masm.emit_mov_r_m(ValueKind.REF, r1, MasmAddr(reg, offsets.X86_64Memory_limit));
		masm.emit_subw_r_r(r1, r2);
		masm.emit_shrw_r_i(r1, 16);
		state.push(KIND_I32 | IN_REG, r1, 0);
	}
	def visit_MEMORY_GROW(memory_index: u31) {
		emit_call_runtime_op1(Opcode.MEMORY_GROW, memory_index, false);
	}

	def visit_I32_CONST(val: i32) {
		state.push(KIND_I32 | IS_CONST, NO_REG, val);
	}
	def visit_I64_CONST(val: i64) {
		if (i32.view(val) == val) {
			state.push(KIND_I64 | IS_CONST, NO_REG, i32.view(val));
		} else {
			var tos = state.sp;
			var addr = masm.slotAddr(tos);
			masm.emit_mov_m_i(addr, int.view(val));
			masm.emit_mov_m_i(addr.plus(4), int.view(val >> 32));
			state.push(KIND_I64 | IS_STORED, NO_REG, 0);
		}
	}
	def visit_F32_CONST(val: u32) {
		if (val == 0 && config_TrackFloatZeroConst) {
			state.push(KIND_F32 | IS_CONST, NO_REG, 0);
		} else {
			var tos = state.sp;
			masm.emit_mov_m_i(masm.slotAddr(tos), int.view(val));
			state.push(KIND_F32 | IS_STORED, NO_REG, 0);
		}
	}
	def visit_F64_CONST(val: u64) {
		if (val == 0 && config_TrackFloatZeroConst) {
			state.push(KIND_F64 | IS_CONST, NO_REG, 0);
		} else {
			var tos = state.sp;
			var addr = masm.slotAddr(tos);
			masm.emit_mov_m_i(addr, int.view(val));
			masm.emit_mov_m_i(addr.plus(4), int.view(val >> 32));
			state.push(KIND_F64 | IS_STORED, NO_REG, 0);
		}
	}

	def visit_REF_NULL(ht_index: u31) {
		state.push(KIND_REF | IS_CONST, NO_REG, 0);
	}
	def visit_REF_FUNC(func_index: u31) {
		var offsets = masm.getOffsets();
		var reg = allocRegTos(ValueKind.REF);
		emit_load_instance(reg);
		// XXX: skip loading the target function for direct intra-module calls?
		masm.emit_mov_r_m(ValueKind.REF, reg, MasmAddr(reg, offsets.Instance_functions));
		masm.emit_read_v3_array_r_i(ValueKind.REF, reg, reg, int.!(func_index));
		state.push(KIND_REF | IN_REG, reg, 0);
	}
	def visit_REF_AS_NON_NULL() {
		var sv = state.peek();
		if (sv.isConst()) {
			if (sv.const == 0) {
				masm.emit_br(masm.newTrapLabel(TrapReason.NULL_DEREF)); // statically null
			} else {
				// nop
			}
		} else if (sv.inReg()) {
			masm.emit_br_r(sv.reg, MasmBrCond.REF_NULL, masm.newTrapLabel(TrapReason.NULL_DEREF));
		} else {
			// XXX: use masm.br_m if necessary?
			var tos = state.sp - 1;
			var reg = allocReg(ValueKind.REF, tos);
			masm.emit_mov_r_s(ValueKind.REF, reg, tos);
			masm.emit_br_r(reg, MasmBrCond.REF_NULL, masm.newTrapLabel(TrapReason.NULL_DEREF));
			state.overwrite(sv.kindFlagsAndTag(IN_REG | (sv.flags & IS_STORED)), reg, 0);
		}
	}
	def visit_EXTERN_INTERNALIZE() { } // nop
	def visit_EXTERN_EXTERNALIZE() { } // nop

	def visit_MEMORY_INIT(dindex: u31, mindex: u31) {
		emit_call_runtime_op2(Opcode.MEMORY_INIT, dindex, mindex, true);
	}
	def visit_DATA_DROP(dindex: u31) {
		var offsets = masm.getOffsets();
		var tmp = allocTmp(ValueKind.REF);
		emit_load_instance(tmp);
		masm.emit_mov_r_m(ValueKind.REF, tmp, MasmAddr(tmp, offsets.Instance_dropped_data));
		var addr = MasmAddr(tmp, offsets.Array_contents + dindex);
		masm.emit_mov_m_i(addr, 1);
	}
	def visit_MEMORY_COPY(mindex1: u31, mindex2: u31) {
		emit_call_runtime_op2(Opcode.MEMORY_COPY, mindex1, mindex2, true);
	}
	def visit_MEMORY_FILL(mindex: u31) {
		emit_call_runtime_op1(Opcode.MEMORY_FILL, mindex, true);
	}
	def visit_TABLE_INIT(eindex: u31, tindex: u31) {
		emit_call_runtime_op2(Opcode.TABLE_INIT, eindex, tindex, true);
	}
	def visit_ELEM_DROP(dindex: u31) {
		var offsets = masm.getOffsets();
		var tmp = allocTmp(ValueKind.REF);
		emit_load_instance(tmp);
		masm.emit_mov_r_m(ValueKind.REF, tmp, MasmAddr(tmp, offsets.Instance_dropped_elems));
		var addr = MasmAddr(tmp, offsets.Array_contents + dindex);
		masm.emit_mov_m_i(addr, 1);
	}
	def visit_TABLE_COPY(tindex1: u31, tindex2: u31) {
		emit_call_runtime_op2(Opcode.TABLE_COPY, tindex1, tindex2, true);
	}
	def visit_TABLE_GROW(index: u31) {
		emit_call_runtime_op1n(Opcode.TABLE_GROW, index, 2, [ValueType.I32], false);
	}
	def visit_TABLE_SIZE(table_index: u31) {
		var offsets = masm.getOffsets();
		var tmp = allocTmp(ValueKind.REF);
		var r1 = allocRegTos(ValueKind.I32);
		emit_load_instance(tmp);
		masm.emit_mov_r_m(ValueKind.REF, tmp, MasmAddr(tmp, offsets.Instance_tables));
		masm.emit_read_v3_array_r_i(ValueKind.REF, tmp, tmp, int.!(table_index));
		masm.emit_mov_r_m(ValueKind.REF, tmp, MasmAddr(tmp, offsets.Table_elems));
		masm.emit_read_v3_array_length_r_r(r1, tmp);
		state.push(KIND_I32 | IN_REG, r1, 0);
	}
	def visit_TABLE_FILL(index: u31) {
		emit_call_runtime_op1n(Opcode.TABLE_FILL, index, 3, ValueTypes.NONE, true);
	}

	def emit_call_runtime_op1(op: Opcode, arg1: u31, canTrap: bool) {
		emit_call_runtime_op1n(op, arg1, op.sig.params.length, op.sig.results, canTrap);
	}
	def emit_call_runtime_op2(op: Opcode, arg1: u31, arg2: u31, canTrap: bool) {
		emit_call_runtime_op2n(op, arg1, arg2, op.sig.params.length, op.sig.results, canTrap);
	}
	def emit_call_runtime_op1n(op: Opcode, arg1: u31, args: int, results: Array<ValueType>, canTrap: bool) {
		state.emitKill(resolver);
		emit_compute_vsp(regs.vsp, state.sp);
		masm.emit_store_runtime_vsp(regs.vsp);
		emit_load_instance(regs.runtime_arg_0);
		masm.emit_mov_r_i(regs.runtime_arg_1, arg1);
		masm.emit_call_runtime_op(op);
		if (canTrap) emit_unwind_check();
		for (i < args) drop();
		for (t in results) state.push(typeToKindFlags(t) | TAG_STORED | IS_STORED, NO_REG, 0);
		emit_reload_regs();
	}
	def emit_call_runtime_op2n(op: Opcode, arg1: u31, arg2: u31, args: int, results: Array<ValueType>, canTrap: bool) {
		state.emitKill(resolver);
		emit_compute_vsp(regs.vsp, state.sp);
		masm.emit_store_runtime_vsp(regs.vsp);
		emit_load_instance(regs.runtime_arg_0);
		masm.emit_mov_r_i(regs.runtime_arg_1, arg1);
		masm.emit_mov_r_i(regs.runtime_arg_2, arg2);
		masm.emit_call_runtime_op(op);
		if (canTrap) emit_unwind_check();
		for (i < args) drop();
		for (t in results) state.push(typeToKindFlags(t) | TAG_STORED | IS_STORED, NO_REG, 0);
		emit_reload_regs();
	}
	def emitBrIf(sv: SpcVal, cond: MasmBrCond, label: MasmLabel, target: SpcControl, emptyTransfer: bool) {
		if (sv.isConst()) {
			var taken = (sv.const == 0) == cond.zero;
			if (taken) {
				if (!emptyTransfer) state.emitTransfer(target, resolver);
				masm.emit_br(label);
			}
		} else if (emptyTransfer) {
			if (sv.inReg()) masm.emit_br_r(sv.reg, cond, label);
			else masm.emit_br_m(masm.slotAddr(state.sp), cond, label);
		} else {
			var skip = masm.newLabel(it.pc);
			if (sv.inReg()) masm.emit_br_r(sv.reg, masm.negate(cond), skip);
			else masm.emit_br_m(masm.slotAddr(state.sp), masm.negate(cond), skip);
			state.emitTransfer(target, resolver);
			masm.emit_br(label);
			masm.bindLabel(skip);
		}
	}
	def emitBrTable(sv: SpcVal, depths: Range<u31>) {
		if (sv.isConst()) {
			// constant-fold br_table into a br
			var key = sv.const;
			if (u32.view(key) >= depths.length) key = depths.length - 1;
			var target = state.getControl(depths[key]);
			state.emitTransfer(target, resolver);
			masm.emit_br(target.label);
			return;
		}
		var labels = Array<MasmLabel>.new(state.ctl_stack.top);
		var targets = Array<MasmLabel>.new(depths.length);
		for (i < targets.length) { // create labels for all targets involved in this br_table
			var depth = depths[i];
			var l = labels[depth];
			if (l == null) l = labels[depth] = masm.newLabel(it.pc);
			targets[i] = l;
		}
		masm.emit_br_table_r(ensureReg(sv, state.sp), targets);

		for (depth < labels.length) {
			var l = labels[depth];
			if (l == null) continue;
			masm.bindLabel(l);
			var target = state.getControl(u32.view(depth));
			state.emitTransfer(target, resolver);
			masm.emit_br(target.label);
		}
	}
	def emitReturn(ctl: SpcControl) {
		// All explicit RETURN instructions branch here.
		if (ret_label != null) {
			masm.bindLabel(ret_label);
			ret_label = null;
		}
		var results = sig.results;
		if (masm.valuerep.tagged) {
			// update mismatched value tags
			var params = sig.params;
			for (i < results.length) {
				var rtag = toTag(results[i]);
				if (i < params.length && rtag == toTag(params[i])) continue; // tag already correct
				masm.emit_mov_m_i(masm.tagAddr(u32.view(i)), rtag.code);
			}
		}
		// Compute VSP = VFP + sig.results.length
		emit_compute_vsp(regs.vsp, u32.view(results.length));
		// Return to caller
		masm.emit_mov_r_i(regs.ret_Abrupt, 0);
		// Deallocate stack frame
		if (unwind_label != null) {
			masm.bindLabel(unwind_label);
			unwind_label = null;
		}
		emitUnwind();
	}
	def emitUnwind() {
		masm.emit_addw_r_i(regs.sp, config.spcFrameSize);
		masm.emit_ret();
	}
	def emitTrap(reason: TrapReason) {
		var label = masm.newTrapLabel(reason);
		masm.emit_br(label);
	}
	def emitTrapReturn(label: MasmLabel, reason: TrapReason) {
		if (label != null) masm.bindLabel(label);
		masm.emit_mov_r_trap(regs.ret_Abrupt, reason);
		masm.emit_addw_r_i(regs.sp, config.spcFrameSize);
		masm.emit_ret();
	}
	def unsupported() {
		success = false; // XXX: add opcode
	}
	def bailout(msg: string) {
		success = false;
		if (Trace.compiler) Trace.OUT.put1("------------ bailout: %s", msg).outln();
		var cp = it.immptr();
		err.rel(cp, it.pc).FailedToCompile(func.func_index, msg);
	}

	// Fold an unary operation if a constant is on the top of the stack.
	def tryFold_i_i(f: i32 -> i32) -> bool {
		var sv = state.peek();
		if (sv.isConst()) {
			var r = f(sv.const);
			if (r != sv.const) {
				pop();
				state.push(KIND_I32 | IS_CONST, NO_REG, r);
			}
			return true;
		}
		return false;
	}
	def tryFold_x_y<X, Y>(kind: ValueKind, f: X -> Y, toX: i32 -> X, toY: i32 -> Y, fromY: Y -> i32) -> bool {
		var a = state.peek();
		if (a.isConst()) {
			var rY = f(toX(a.const));
			var r = fromY(rY);
			var cY = toY(r);
			if (cY != rY) return false;
			pop();
			state.push(SpcConsts.kindToFlags(kind) | IS_CONST, NO_REG, r);
			return true;
		}
		return false;
	}
	def tryFold_u_u(f: u32 -> u32) -> bool { return tryFold_x_y(ValueKind.I32, f, u32.view<i32>, u32.view<i32>, i32.view<u32>); }
	def tryFold_i_l(f: i32 -> i64) -> bool { return tryFold_x_y(ValueKind.I64, f, i32.view<i32>, i64.view<i32>, i32.view<i64>); }
	def tryFold_u_l(f: u32 -> i64) -> bool { return tryFold_x_y(ValueKind.I64, f, u32.view<i32>, i64.view<i32>, i32.view<i64>); }
	def tryFold_q_q(f: u64 -> u64) -> bool { return tryFold_x_y(ValueKind.I64, f, u64.view<i32>, u64.view<i32>, i32.view<u64>); }
	def tryFold_l_i(f: i64 -> i32) -> bool { return tryFold_x_y(ValueKind.I32, f, i64.view<i32>, i32.view<i32>, i32.view<i32>); }
	def tryFold_l_l(f: i64 -> i64) -> bool { return tryFold_x_y(ValueKind.I64, f, i64.view<i32>, i64.view<i32>, i32.view<i64>); }

	// Utilities to try to constant-fold using the given evaluation function {f}. Since all SPC constants
	// are represented by the V3 type {i32}, this is the fast path.
	def tryFold_ii_i(f: (i32, i32) -> i32) -> bool {
		var sv = state.peek2(), a = sv.0, b = sv.1;
		if (a.isConst() && b.isConst()) {
			var r = f(a.const, b.const);
			pop();
			pop();
			state.push(KIND_I32 | IS_CONST, NO_REG, r);
			return true;
		}
		return false;
	}
	// For evaluation functions that are not of type {i32}, adapt them polymorphically.
	def tryFold_xx_y<X, Y>(kind: ValueKind, f: (X, X) -> Y, toX: i32 -> X, toY: i32 -> Y, fromY: Y -> i32) -> bool {
		var sv = state.peek2(), a = sv.0, b = sv.1;
		if (a.isConst() && b.isConst()) {
			var rY = f(toX(a.const), toX(b.const));
			var r = fromY(rY);
			var cY = toY(r);
			if (cY != rY) return false;
			pop();
			pop();
			state.push(SpcConsts.kindToFlags(kind) | IS_CONST, NO_REG, r);
			return true;
		}
		return false;
	}
	// All the polymorphic variants of folding.
	def tryFold_uu_u(f: (u32, u32) -> u32) -> bool   { return tryFold_xx_y(ValueKind.I32, f, u32.view<i32>, u32.view<i32>, i32.view<u32>); }
	def tryFold_ii_z(f: (i32, i32) -> bool) -> bool  { return tryFold_xx_y(ValueKind.I32, f, i32.view<i32>, isNotZero, trueToOne); }
	def tryFold_uu_z(f: (u32, u32) -> bool) -> bool  { return tryFold_xx_y(ValueKind.I32, f, u32.view<i32>, isNotZero, trueToOne); }
	def tryFold_ll_l(f: (i64, i64) -> i64) -> bool   { return tryFold_xx_y(ValueKind.I64, f, i64.view<i32>, i64.view<i32>, i32.view<i64>); }
	def tryFold_qq_q(f: (u64, u64) -> u64) -> bool   { return tryFold_xx_y(ValueKind.I64, f, u64.view<i32>, u64.view<i32>, i32.view<u64>); }
	def tryFold_ll_z(f: (i64, i64) -> bool) -> bool  { return tryFold_xx_y(ValueKind.I32, f, i64.view<i32>, isNotZero, trueToOne); }
	def tryFold_qq_z(f: (u64, u64) -> bool) -> bool  { return tryFold_xx_y(ValueKind.I32, f, u64.view<i32>, isNotZero, trueToOne); }

	//====================================================================
	// codegen operations
	//====================================================================
	def emit_unwind_check() {
		if (unwind_label == null) unwind_label = masm.newLabel(func.cur_bytecode.length);
		masm.emit_brne_r_i(regs.ret_Abrupt, 0, unwind_label);
	}
	def emit_compute_vsp(dst: Reg, slots: u32) {
		masm.emit_mov_r_r(dst, regs.vfp); // XXX: use 3-addr adjustment of VSP (i.e. lea)
		if (slots > 0) masm.emit_addw_r_i(dst, int.view(slots) * masm.valuerep.slot_size);
	}
	def emit_reload_regs() {
		// XXX: recompute VFP from VSP - #slots?
		masm.emit_mov_r_m(ValueKind.REF, regs.vfp, MasmAddr(regs.sp, config.vfp_offset));
		if (module.memories.length > 0) {
			masm.emit_mov_r_m(ValueKind.REF, regs.mem0, MasmAddr(regs.sp, config.mem_offset));
		}
	}
	def emit_load_instance(reg: Reg) {
		masm.emit_mov_r_m(ValueKind.REF, reg, MasmAddr(regs.sp, config.instance_offset));
	}
	def emitLoad(kind: ValueKind, imm: MemArg, meth: (ValueKind, Reg, Reg, Reg, u32) -> ())  {
		var base_reg = regs.mem0;
		if (imm.memory_index != 0) {
			// XXX: cache the base register for memories > 0
			var offsets = masm.getOffsets();
			base_reg = allocTmp(ValueKind.REF);
			emit_load_instance(base_reg);
			masm.emit_mov_r_m(ValueKind.REF, base_reg, MasmAddr(base_reg, offsets.Instance_memories));
			masm.emit_read_v3_array_r_i(ValueKind.REF, base_reg, base_reg, imm.memory_index);
			masm.emit_read_v3_mem_base(base_reg, base_reg);
		}
		var iv = pop();
		var index_reg: Reg;
		var offset = imm.offset;
		if (iv.isConst()) {
			var sum = u64.view(offset) + u32.view(iv.const); // fold offset calculation
			if (sum > u32.max) {
				masm.emit_br(masm.newTrapLabel(TrapReason.MEM_OUT_OF_BOUNDS)); // statically OOB
				setUnreachable();
				return;
			}
			offset = u32.view(sum);
		} else {
			index_reg = ensureReg(iv, state.sp);
		}
		var dest = allocRegTos(kind); // XXX: can reuse index reg if frequency == 1 and ValueKind.I32
		meth(kind, dest, base_reg, index_reg, u32.!(offset)); // TODO: memory64
		var nflags = IN_REG | SpcConsts.kindToFlags(kind);
		if (kind == ValueKind.I32) nflags |= (iv.flags & TAG_STORED); // tag may already be stored for index
		state.push(nflags, dest, 0);
	}
	def emitStore(kind: ValueKind, imm: MemArg, meth: (ValueKind, Reg, Reg, Reg, u32) -> ()) {
		var base_reg = regs.mem0;
		if (imm.memory_index != 0) {
			// XXX: cache the base register for memories > 0
			var offsets = masm.getOffsets();
			base_reg = allocTmp(ValueKind.REF);
			emit_load_instance(base_reg);
			masm.emit_mov_r_m(ValueKind.REF, base_reg, MasmAddr(base_reg, offsets.Instance_memories));
			masm.emit_read_v3_array_r_i(ValueKind.REF, base_reg, base_reg, imm.memory_index);
			masm.emit_read_v3_mem_base(base_reg, base_reg);
		}
		var offset = imm.offset;
		var sv = popReg();
		var iv = state.peek();
		var index_reg: Reg;
		if (iv.isConst()) {
			var sum = u64.view(offset) + u32.view(iv.const); // fold offset calculation
			if (sum > u32.max) {
				masm.emit_br(masm.newTrapLabel(TrapReason.MEM_OUT_OF_BOUNDS)); // statically OOB
				setUnreachable();
				return;
			}
			offset = u32.view(sum);
			pop();
		} else {
			index_reg = popReg().reg;
		}
		meth(kind, sv.reg, base_reg, index_reg, u32.!(offset)); // TODO: memory64
	}

	//====================================================================
	// register allocation operations
	//====================================================================
	def freeReg(reg: Reg) {
		if (reg.index > 0) masm.regAlloc.free(reg);
	}
	def allocRegTos(kind: ValueKind) -> Reg {
		return allocReg(kind, state.sp);
	}
	def allocReg(kind: ValueKind, slot: u32) -> Reg {
		var reg = masm.regAlloc.alloc(kind, int.!(slot));
		if (reg == NO_REG) return findAndSpillReg(kind, int.!(slot));
		return reg;
	}
	def allocTmp(kind: ValueKind) -> Reg {
		var reg = masm.regAlloc.alloc(kind, TMP_SLOT);
		if (reg == NO_REG) reg = findAndSpillReg(kind, TMP_SLOT);
		unrefLater(reg, TMP_SLOT);
		return reg;
	}
	def allocTmpFixed(kind: ValueKind, reg: Reg) -> Reg {
		if (masm.regAlloc.isFree(reg)) {
			masm.regAlloc.assign(reg, TMP_SLOT);
		} else {
			spillReg(reg);
			reassignReg(reg, TMP_SLOT);
		}
		unrefLater(reg, TMP_SLOT);
		return reg;
	}
	def ensureReg(sv: SpcVal, slot: u32) -> Reg {
		var reg = sv.reg;
		if (reg == NO_REG) {
			var kind = sv.kind();
			reg = allocTmp(kind);
			if (sv.isConst()) masm.emit_mov_r_k(kind, reg, sv.const);
			else masm.emit_mov_r_s(kind, reg, slot);
		}
		return reg;
	}
	def unrefRegs() {
		var c = num_unrefs;
		for (i < c) masm.regAlloc.unassign(unrefs[i]);
		num_unrefs = 0;
	}
	def unrefSlot(slot: u32) {
		var sv = state.state[slot];
		if (sv.reg != NO_REG) masm.regAlloc.unassign(sv.reg, int.!(slot));
	}
	def unrefLater(reg: Reg, assign: int) { // XXX: #inline
		unrefs[num_unrefs++] = (reg, assign);
	}
	def findAndSpillReg(kind: ValueKind, slot: int) -> Reg {
		var reg = masm.regAlloc.findSpillCandidate(kind, addSpillCost);
		if (reg != NO_REG) {
			spillReg(reg);
			reassignReg(reg, slot);
			return reg;
		}
		bailout("out of registers");
		masm.regAlloc.clear();
		return masm.regAlloc.alloc(kind, slot);
	}
	def addSpillCost(score: int, reg: Reg, slot: int) -> int {
		if (!state.state[slot].isStored()) score += 100;      // will generate a spill
		if (slot < 10 && slot < func.num_locals) score += 10; // penalize first 10 locals
		if (int.!(state.sp) - slot < 4) score += 20;          // penalize top 4 operand slots
		// XXX: boost first 8 x86 registers because of possible REX byte?
		return score;
	}

	//====================================================================
	// abstract stack operations
	//====================================================================
	def labelArgs(ctl: SpcControl) -> Array<ValueType> {
		if (ctl.opcode == Opcode.LOOP.code) return ctl.params;
		else return ctl.results;
	}
	// Pop the top of the stack.
	def pop() -> SpcVal {
		var sv = state.pop();
		if (sv.inReg()) unrefLater(sv.reg, int.!(state.sp));
		return sv;
	}
	// Pop and drop the top of the stack.
	def drop() {
		var sv = state.pop();
		if (sv.reg != NO_REG) masm.regAlloc.unassign(sv.reg, int.!(state.sp));
	}
	// Pop the top of the stack into a register of the appropriate kind.
	def popReg() -> SpcVal {
		var sv = state.pop();
		if (sv.inReg()) {
			unrefLater(sv.reg, int.!(state.sp));
			return sv;
		}
		var kind = sv.kind();
		var reg = allocTmp(kind);
		if (sv.isConst()) masm.emit_mov_r_k(kind, reg, sv.const);
		else masm.emit_mov_r_s(kind, reg, state.sp);
		unrefLater(sv.reg, int.!(state.sp));
		return SpcVal(sv.flags | IN_REG, reg, sv.const);
	}
	// Pop the top of the stack into {reg}, spilling any value(s) in {reg} first.
	def popFixedReg(reg: Reg) -> SpcVal {
		var sv = state.pop(), slot = int.!(state.sp);
		if (sv.reg == reg) {
			unrefLater(sv.reg, slot);
			return sv;
		}
		spillReg(reg);
		masm.regAlloc.assign(reg, slot);
		var kind = sv.kind();
		if (sv.inReg()) {
			masm.emit_mov_r_r(reg, sv.reg);
			masm.regAlloc.unassign(sv.reg, slot);
		} else if (sv.isConst()) {
			masm.emit_mov_r_k(kind, reg, sv.const);
		} else {
			masm.emit_mov_r_s(kind, reg, state.sp);
		}
		unrefLater(reg, slot);
		return SpcVal(sv.flags | IN_REG, reg, sv.const);
	}
	def reassignReg(reg: Reg, slot: int) {
		masm.regAlloc.free(reg);
		masm.regAlloc.assign(reg, slot);
	}
	// Pop the top of stack into a register and prepare for it to be overwritten.
	def popRegToOverwrite() -> SpcVal {
		var sv = state.pop();
		if (masm.regAlloc.frequency(sv.reg) == 1) return sv; // can reuse reg immediately
		var kind = sv.kind();
		var reg = allocRegTos(kind);
		if (sv.isConst()) {
			masm.emit_mov_r_k(kind, reg, sv.const);
		} else if (sv.reg != NO_REG) {
			masm.emit_mov_r_r(reg, sv.reg); // XXX: more efficient move with kind?
			masm.regAlloc.unassign(sv.reg, int.!(state.sp));
		} else {
			masm.emit_mov_r_s(kind, reg, state.sp);
		}
		return SpcVal(sv.flags | IN_REG, reg, sv.const);
	}
	// Spill a register to its slot(s), if it has been allocated.
	def spillReg(reg: Reg) {
		masm.regAlloc.forEachAssignment(reg, emitSpill); // XXX: iterate assignment list directly?
	}
	def emitSpill(reg: Reg, slot: int) {
		if (slot == TMP_SLOT) return bailout(Strings.format1("attempt to spill %s, assigned as temp", config.regSet.getName(reg)));
		var sv = state.state[slot];
		if (sv.reg != reg) bailout(Strings.format2("spill mismatch for %s, assignment=%d", config.regSet.getName(reg), slot));
		if (sv.isConst()) return void(state.state[slot] = sv.withoutReg());
		if (sv.isStored()) return void(state.state[slot] = sv.withoutReg());
		masm.emit_mov_s_r(sv.kind(), u32.!(slot), reg);
		state.state[slot] = SpcVal((sv.flags | IS_STORED) & ~IN_REG, NO_REG, 0);
	}
	def setUnreachable() {
		skip_to_end = true;
		state.setUnreachable();
	}
	def traceOpcodeAndStack() {
		OUT.out(Trace.STDOUT);
		OUT.put2("  %x(+%d): ", it.pc, it.pc - start_pos);
		it.trace(OUT, module, instrTracer);
		while (OUT.length < 32) OUT.puts(" ");
		state.trace();
	}
	def traceOpcodeUnreachable() {
		OUT.out(Trace.STDOUT);
		OUT.put2("  %x(+%d): ", it.pc, it.pc - start_pos);
		it.trace(OUT, module, instrTracer);
		while (OUT.length < 32) OUT.puts(" ");
		OUT.puts("...");
		OUT.outln();
	}
}

// Compiler representation of (abstract) values on the WebAssembly value stack,
// including its {ValueKind}, whether the value is stored into the corresponding slot,
// whether the value tag (if tags enabled) is stored into the slot, which if any
// register holds its value, and which constant value, if any, it is.
type SpcVal(flags: byte, reg: Reg, const: int) #unboxed {
	def kind() -> ValueKind {
		return SpcConsts.kinds[flags >> 4];
	}
	def kindFlags(add: byte) -> byte {
		return (flags & KIND_MASK) | add;
	}
	def kindFlagsAndTag(add: byte) -> byte {
		return (flags & (KIND_MASK | TAG_STORED)) | add;
	}
	def kindFlagsMatching(kind: ValueKind, add: byte) -> byte {
		var okind = (flags & KIND_MASK);
		var nkind = SpcConsts.kindToFlags(kind);
		if (okind == nkind) nkind |= (flags & TAG_STORED);
		return nkind | add;
	}
	def isStored() -> bool {
		return (flags & IS_STORED) != 0;
	}
	def isConst() -> bool {
		return (flags & IS_CONST) != 0;
	}
	def inReg() -> bool {
		return (flags & IN_REG) != 0;
	}
	def tagStored() -> bool {
		return (flags & TAG_STORED) != 0;
	}
	def withoutReg() -> SpcVal {
		return SpcVal(flags & ~IN_REG, NO_REG, const);
	}
	def render(buf: StringBuilder, regSet: RegSet) -> StringBuilder {
		match (kind()) {
			I32 => buf.puts("i");
			I64 => buf.puts("l");
			F32 => buf.puts("f");
			F64 => buf.puts("d");
			V128 => buf.puts("v");
			REF => buf.puts("r");
			ABS => buf.puts("a");
		}
		buf.put2("%s%s",
			if(tagStored(), "T", ""),
			if(isStored(), "S", ""));
		if (inReg()) buf.put1("@%s", regSet.getName(reg));
		if (isConst()) buf.put1("=%d", const);
		buf.puts("|");
		return buf;
	}
}

// An entry in the abstract control stack.
class SpcControl {
	var opcode: byte;
	var params: Array<ValueType>;
	var results: Array<ValueType>;
	var reachable = true;
	var val_stack_top: u32;
	var label: MasmLabel;
	var else_label: MasmLabel;
	// the state at the merge (label)
	var merge_count: byte;
	var merge_state: Array<SpcVal>;
	// the state used to reset back to before the true branch of an if
	var reset_state: Array<SpcVal>;

	def clearMerge() {
		merge_count = 0;
		merge_state = null;
	}
	def clearReset() {
		reset_state = null;
	}
}

def isZero = int.==(0, _);
def isOne = int.==(1, _);
def isNotZero = int.!=(0, _);
def trueToOne(z: bool) -> int { return if(z, 1, 0); }
def isMinusOne = int.==(-1, _);
def to_ii_i(f: (u32, u32) -> u32, a: int, b: int) -> int {
	return int.view(f(u32.view(a), u32.view(b)));
}
def i32_unsigned_to_i64(i: i32) -> i64 { return i64.view(u32.view(i)); }
def I32_ROTL = to_ii_i(V3Eval.I32_ROTL, _, _);
def I32_ROTR = to_ii_i(V3Eval.I32_ROTR, _, _);

// Contains both the abstract control and abstract value stack.
class SpcState(regAlloc: RegAlloc) {
	// Abstract state of the value stack
	var state = Array<SpcVal>.new(INITIAL);
	var sp: u32;
	var ctl_stack = ArrayStack<SpcControl>.new();
	var num_locals: u16;

	// Reset the state for starting a new function.
	def reset(sig: SigDecl, ret_label: MasmLabel) {
		sp = 0;
		ctl_stack.clear();
		// manually set up first control entry and return merge state
		var results = sig.results;
		var ctl = pushControl(Opcode.RETURN.code, ValueTypes.NONE, results, ret_label);
		var merge_state = Array<SpcVal>.new(results.length);
		for (i < results.length) {
			// request the merged values be stored to the stack, but don't require tags
			merge_state[i] = SpcVal(typeToKindFlags(results[i]) | IS_STORED, NO_REG, 0);
		}
		ctl.merge_state = merge_state;
		ctl.merge_count = 1;
		// initialize parameters
		var params = sig.params;
		grow(params.length);
		for (i < params.length) {
			// params start on the stack and already have tags
			state[i] = SpcVal(typeToKindFlags(params[i]) | TAG_STORED | IS_STORED, NO_REG, 0);
		}
		sp = u32.view(params.length);
	}
	// Add the specified number of locals of the specified type.
	def addLocals(count: u32, ltype: ValueType) {
		var nlength = sp + count;
		if (nlength > state.length) grow(int.view(nlength + sp * 2));
		var flags = typeToKindFlags(ltype) | TAG_STORED | IS_CONST; // TODO: store tag of locals
		for (j < count) {
			var k = j + sp;
			state[k] = SpcVal(flags, NO_REG, 0);
		}
		sp = nlength;
	}
	def pushBlock(params: Array<ValueType>, results: Array<ValueType>, end_label: MasmLabel) -> SpcControl {
		return pushControl(Opcode.BLOCK.code, params, results, end_label);
	}
	def pushLoop(params: Array<ValueType>, results: Array<ValueType>, start_label: MasmLabel) -> SpcControl {
		var ctl = pushControl(Opcode.LOOP.code, params, results, start_label);
		return ctl;
	}
	def prepareLoop(resolver: SpcMoveResolver) {
		var target = ctl_stack.peek();
		target.merge_count = 1;
		var val_count = u32.view(target.params.length);
		target.merge_state = doFirstTransferAndGetMerge(sp - val_count, val_count, resolver);
		resetTo(sp, target.merge_state);
	}
	// Prepare for the first transfer to a label by revoking registers/constants and emitting
	// any necessary moves. Here, {L} refers to locals, {Z} to the frozen portion of the operand
	// stack, and {F} to the arguments to the loop. The current state is *not* updated, but
	// the new target merge state is returned.
	//   | L | Z | T | ... | F |
	//     |                 |
	//     |       +---------+
	//     |       |
	//     v       v
	//   |!L!| Z |!F!|
	def doFirstTransferAndGetMerge(sp: u32, val_count: u32, resolver: SpcMoveResolver) -> Array<SpcVal> {
		var merge = Arrays.range(state, 0, int.view(sp + val_count));
//			for (i < top) {
//				var from = state[i], to = toMergeVal(from);
//				target.merge_state[i] = to;
//				if (from != to) resolver.addMove((i, to), (i, from));
//			}
//			for (i < vals) {
//				var f = (sp - vals + i), t = top + i;
//				var from = state[f], to = toMergeVal(from);
//				target.merge_state[t] = to;
//				resolver.addMove((t, to), (f, from));
//			}
		return merge;
	}
	// Prepare the current state for the transfer of a true select by revoking registers/constants and emitting
	// any necessary moves. Here, {L} refers to locals, {Z} to the frozen portion of the operand
	// stack, and {F} to the arguments to the loop.
	//   | L | Z | A | B |
	//             |
	//             +
	//             |
	//             v
	//   | L | Z |!F!|
	def doSelectTrueTransferInPlace(val_count: u32, resolver: SpcMoveResolver) {
//		state.prepareRangeForMerge(base, base + count, resolver);
	}
	// Prepare the current state for the transfer of a false select by revoking registers/constants and emitting
	// any necessary moves. Here, {L} refers to locals, {Z} to the frozen portion of the operand
	// stack, and {F} to the arguments to the loop.
	//   | L | Z | A | B |
	//                 |
	//             +---+
	//             |
	//             v
	//   | L | Z |!F!|
	def doSelectFalseTransferInPlace(val_count: u32, resolver: SpcMoveResolver) {
//		for (i = base; i < base + count; i++) {
//			var fv = state.state[i + count];
//			resolver.addMove((i, state.state[i]), (i + count, fv));
//			unrefSlot(i + count);
//		}
	}
	def prepareRangeForMerge(start: u32, end: u32, resolver: SpcMoveResolver) {
		for (i = start; i < end; i++) { // XXX: iterate top of stack first?
			var from = state[i];
			if (from.reg != NO_REG && regAlloc.frequency(from.reg) > 1) {
				// A register used in the range can only be used once. Spill other uses.
				regAlloc.forEachAssignment(from.reg, spillIfNotEqual(_, _, i, resolver));
			}
			var to = toMergeVal(from);
			if (from != to) {
				state[i] = to;
				resolver.addMove((i, to), (i, from));
			}
		}
		resolver.emitMoves();
	}
	def spillIfNotEqual(reg: Reg, assign: int, slot: u32, resolver: SpcMoveResolver) {
		if (assign == slot) return;
		var from = state[slot], to: SpcVal;
		if (from.isStored()) {
			// No need to spill, just revoke the register.
			to = from.withoutReg();
		} else {
			to = SpcVal(from.kindFlagsAndTag(IS_STORED), NO_REG, 0);
			resolver.addMove((slot, to), (slot, from));
		}
		state[slot] = to;
	}
	def pushIf(params: Array<ValueType>, results: Array<ValueType>, else_label: MasmLabel, end_label: MasmLabel) -> SpcControl {
		var ctl = pushControl(Opcode.IF.code, params, results, end_label);
		ctl.else_label = else_label;
		ctl.reset_state = Arrays.dup(state);
		return ctl;
	}
	def doElse() {
		var ctl = ctl_stack.peek();
		ctl.else_label = null;
		// reset state to start of if
		var max = ctl.val_stack_top + u32.view(ctl.params.length);
		resetTo(max, ctl.reset_state);
		ctl.clearReset();
		if (ctl_stack.top > 1) ctl.reachable = ctl_stack.elems[ctl_stack.top - 2].reachable;
		else ctl.reachable = true;
	}
	def resetToMerge(ctl: SpcControl) {
		if (ctl.merge_count > 0) {
			var max = ctl.val_stack_top + u32.view(ctl.results.length);
			resetTo(max, ctl.merge_state);
		} else {
			// merge not reached; push "bottom" value for all results
			sp = ctl.val_stack_top;
			for (r in ctl.results) {
				push(typeToKindFlags(r) | IS_STORED | TAG_STORED | IS_CONST, NO_REG, 0);
			}

		}
	}
	def isTransferEmpty(target: SpcControl) -> bool {
		return false; // XXX: approximate
	}
	def emitFallthru(resolver: SpcMoveResolver) {
		emitTransfer(ctl_stack.peek(), resolver);
	}
	def emitTransfer(target: SpcControl, resolver: SpcMoveResolver) {
		if (!ctl_stack.peek().reachable) {
			if (Trace.compiler) OUT.puts("    xfer not reachable").outln();
			return; // do nothing
		}
		var vals = u32.view(if(target.opcode == Opcode.LOOP.code, target.params, target.results).length);
		var top = target.val_stack_top, max = top + vals;
		if (Trace.compiler) OUT.put2("    xfer -> sp=%d vals=%d", top, vals).outln();
		if (target.merge_count == 0) {
			if (Trace.compiler) OUT.puts("    merge_count=1").outln();
			target.merge_count = 1;
			target.merge_state = doFirstTransferAndGetMerge(target.val_stack_top, vals, resolver);
		} else {
			if (Trace.compiler) OUT.puts("    merge_count=2+").outln();
			target.merge_count = 2;
			// XXX: allow matching constants in merges
			for (i < top) {
				var from = state[i], to = target.merge_state[i];
				if (from != to) resolver.addMove((i, to), (i, from));
			}
			for (i < vals) {
				var f = (sp - vals + i), t = top + i;
				var from = state[f], to = target.merge_state[t];
				// TODO: write tag based on mismatch on this stack's target slot
				resolver.addMove((t, to), (f, from));
			}
		}
		resolver.emitMoves();
	}
	def emitKill(resolver: SpcMoveResolver) {
		if (Trace.compiler) OUT.puts("    kill all").outln();
		for (i < sp) {
			var sv = state[i], oflags = sv.flags;
			if ((oflags & IN_REG) != 0) regAlloc.free(sv.reg);
			var stored = IS_STORED | TAG_STORED;
			var nflags = (oflags & ~IN_REG) | stored;
			var nv = SpcVal(nflags, NO_REG, sv.const);
			state[i] = nv;
			if (stored == (oflags & stored)) continue;
			var slot = u32.view(i);
			resolver.addMove((slot, nv), (slot, sv));
		}
		resolver.emitMoves();
	}
	private def toMergeVal(from: SpcVal) -> SpcVal {
		// XXX: allow constants in merges
		var force_store = if(!from.inReg(), IS_STORED);
		return SpcVal((from.flags & ~(IS_CONST)) | force_store, from.reg, 0);
	}
	private def resetTo(max: u32, nstate: Array<SpcVal>) {
		regAlloc.clear();
		for (i < max) {
			var sv = nstate[i];
			if (sv.inReg()) regAlloc.assign(sv.reg, int.!(i));
			state[i] = sv;
		}
		sp = max;
	}
	private def pushControl(opcode: byte, params: Array<ValueType>, results: Array<ValueType>, label: MasmLabel) -> SpcControl {
		var ctl = ctl_stack.next();
		var reachable = if(ctl_stack.top > 0, ctl_stack.peek().reachable, true);
		if (ctl != null) { // FAST: reuse previous SpcControl object
			ctl_stack.top++;
			ctl.clearMerge();
			ctl.clearReset();
			ctl.else_label = null;
		} else { // allocate and cache new SpcControl object
			ctl = SpcControl.new();
			ctl_stack.push(ctl);
		}
		ctl.opcode = opcode;
		ctl.label = label;
		ctl.params = params;
		ctl.results = results;
		ctl.val_stack_top = sp - u32.view(params.length);
		ctl.reachable = reachable;
		ctl.merge_count = 0;
		return ctl;
	}
	def getControl(depth: u32) -> SpcControl {
		var result = ctl_stack.elems[ctl_stack.top - int.!(depth) - 1];
		return result;
	}
	def push(flags: byte, reg: Reg, const: int) {
		var sp = this.sp;
		if (sp >= state.length) grow(8 + state.length * 2);
		state[sp] = SpcVal(flags, reg, const);
		this.sp++;
	}
	def pushV(v: SpcVal) {
		var sp = this.sp;
		if (sp >= state.length) grow(8 + state.length * 2);
		state[sp] = v;
		this.sp++;
	}
	def get(slot: u32) -> SpcVal {
		return state[slot];
	}
	def pop() -> SpcVal {
		if (sp == 0) {
// TODO			err.at(codeptr).set("stack underflow");
			var d: SpcVal;
			return d;
		}
		var result = state[--this.sp];
		return result;
	}
	def popArgsAndPushResults(sig: SigDecl) {
		sp -= u32.view(sig.params.length); // note: assume registers have been freed
		for (t in sig.results) {
			push(typeToKindFlags(t) | TAG_STORED | IS_STORED, NO_REG, 0);
		}
	}
	def peek() -> SpcVal {
		return state[sp - 1];
	}
	def peek2() -> (SpcVal, SpcVal) {
		return (state[sp - 2], state[sp - 1]);
	}
	def overwrite(flags: byte, reg: Reg, const: int) {
		var old = state[sp - 1];
		var tag_stored = if((old.flags & KIND_MASK) == (flags & KIND_MASK), old.flags & TAG_STORED);
		state[sp - 1] = SpcVal(tag_stored | flags, reg, const);
	}
	def set(slot: u32, flags: byte, reg: Reg, const: int) {
		state[slot] = SpcVal(flags, reg, const);
	}
	def setStored(slot: u32) {
		var b = state[slot];
		state[slot] = SpcVal(b.flags | IS_STORED, b.reg, b.const);
	}
	def setUnreachable() {
		ctl_stack.peek().reachable = false;
	}
	def setNoReg(slot: u32) {
		var b = state[slot];
		state[slot] = SpcVal(b.flags & ~IN_REG, NO_REG, b.const);
	}
	def grow(nlength: int) {
		state = Arrays.grow(state, nlength);
	}
	def trace() {
		OUT.put1("sp=%d |", sp);
		for (i < sp) state[i].render(OUT, regAlloc.regConfig.regSet);
		OUT.outln();
	}
}
def toTag(vt: ValueType) -> ValueKind {
	match (vt) {
		I32 => return ValueKind.I32;
		I64 => return ValueKind.I64;
		F32 => return ValueKind.F32;
		F64 => return ValueKind.F64;
		V128 => return ValueKind.V128;
		_ => return ValueKind.REF;
	}
}
def typeToKindFlags(vt: ValueType) -> byte {
	match (vt) {
		I32 => return KIND_I32;
		I64 => return KIND_I64;
		F32 => return KIND_F32;
		F64 => return KIND_F64;
		V128 => return KIND_V128;
		_ => return KIND_REF;
	}
}

// Collects the parallel moves and orders them properly.
// Reused repeatedly by the compiler, so nodes are recycled internally.
class SpcMoveResolver(masm: MacroAssembler) {
	private var moves = Array<MoveNode>.new(16);
	private var numMoves = 0;
	private def constRegMoves = Vector<(ValueKind, int, Reg)>.new();
	private def constSlotMoves = Vector<(ValueKind, int, u32)>.new();

	// Add a move or moves to the collection of parallel moves to transfer the
	// abstract value in one slot to another. The abstract value may be a constant,
	// in a register, or in a slot.
	// Generates a value tag store if necessary.
	def addMove(to: (u32, SpcVal), from: (u32, SpcVal)) {
		if (Trace.compiler) {
			OUT.put1("    addMove slot=%d ", to.0);
			to.1.render(OUT, masm.regAlloc.regConfig.regSet);
			OUT.put1(" <- slot=%d ", from.0);
			from.1.render(OUT, masm.regAlloc.regConfig.regSet);
			OUT.outln();
		}
		var tv = to.1, fv = from.1;
		if (masm.valuerep.tagged && tv.tagStored()) { // XXX: move tag store operation into state?
			if (from.0 != to.0 || !fv.tagStored()) {
				// store the tag into to-slot
				masm.emit_mov_m_i(masm.tagAddr(to.0), fv.kind().code);
			}
		}
		if (fv.isConst()) {
			if (tv.isStored()) constSlotMoves.put(fv.kind(), fv.const, to.0);
			if (tv.inReg()) constRegMoves.put(fv.kind(), fv.const, tv.reg);
			return;
		}
		var kind = fv.kind();
		if (tv.isStored()) {
			// store the value into slot
			if (from.0 != to.0 || !fv.isStored()) {
				if (fv.inReg()) mov(kind, s(to.0), r(fv.reg));
				else mov(kind, s(to.0), s(from.0));
			}
		}
		if (tv.inReg()) {
			// load or move the value into appropriate register
			if (fv.inReg()) {
				if (tv.reg != fv.reg) mov(kind, r(tv.reg), r(fv.reg));
			} else {
				mov(kind, r(tv.reg), s(from.0));
			}
		}
	}
	// Order and emit the moves to the macro assembler.
	def emitMoves() {
		for (i < numMoves) {
			var m = moves[i];
			if (m.color == 0) orderMove(null, m);
		}
		numMoves = 0; // resize back to 0, but reuse node objects

		for (i < constRegMoves.length) {
			var m = constRegMoves[i];
			masm.emit_mov_r_k(m.0, m.2, m.1);
		}
		constRegMoves.resize(0);
		for (i < constSlotMoves.length) {
			var m = constSlotMoves[i];
			masm.emit_mov_s_k(m.0, m.2, m.1);
		}
		constSlotMoves.resize(0);
		constRegMoves.resize(0);
	}
	private def mov(kind: ValueKind, dst: MoveNode, src: MoveNode) {
		dst.kind = kind;
		dst.dstNext = src.dstList;
		src.dstList = dst;
		dst.src = src;
	}
	private def s(slot: u32) -> MoveNode {
		for (i < numMoves) { // XXX: avoid linear search for matching MoveNode
			var m = moves[i];
			if (m.slot == slot && m.reg == NO_REG) return m;
		}
		var m = allocMove();
		m.slot = slot;
		m.reg = NO_REG;
		return m;
	}
	private def r(reg: Reg) -> MoveNode {
		for (i < numMoves) { // XXX: avoid linear search for matching MoveNode
			var m = moves[i];
			if (m.reg == reg) return m;
		}
		var m = allocMove();
		m.slot = 0;
		m.reg = reg;
		return m;
	}
	private def allocMove() -> MoveNode {
		if (numMoves >= moves.length) moves = Arrays.grow(moves, moves.length * 2);
		var m = moves[numMoves++];
		if (m == null) {
			m = moves[numMoves - 1] = MoveNode.new();
			m.kind = ValueKind.ABS;
			return m;
		}
		m.kind = ValueKind.ABS;
		m.src = null;
		m.dstList = null;
		m.dstNext = null;
		m.color = 0;
		return m;
	}
	// depth-first recursive traversal of the move graph, inserting moves in post-order
	private def orderMove(alloc: int -> int, node: MoveNode) {
		if (node.color == BLACK) return;
		node.color = GRAY;
		for (l = node.dstList; l != null; l = l.dstNext) {
			if (l.color == GRAY) {
				// cycle detected; break it with temporary register
				var tmp = allocTmp(l.kind);
				emitMove(tmp, l);		// save
				emitMove(l, node);		// overwrite
				l.reg = tmp.reg;		// on-stack uses will use tmp
			} else {
				orderMove(alloc, l);
			}
		}
		node.color = BLACK;
		if (node.src != null) emitMove(node, node.src);
	}
	private def allocTmp(kind: ValueKind) -> MoveNode {
		var m = MoveNode.new(); // XXX: could reuse tmp by kind
		m.kind = kind;
		m.reg = masm.getScratchReg(kind);
		return m;
	}
	private def emitMove(dst: MoveNode, src: MoveNode) {
		if (dst.reg == NO_REG) {
			if (src.reg == NO_REG) {
				if (Trace.compiler) OUT.put3("    emitMove[%s] slot=%d <- slot=%d", dst.kind.name, dst.slot, src.slot).outln();
				masm.emit_mov_s_s(dst.kind, dst.slot, src.slot);
			} else {
				if (Trace.compiler) OUT.put3("    emitMove[%s] slot=%d <- %s", dst.kind.name, dst.slot, name(src.reg)).outln();
				masm.emit_mov_s_r(dst.kind, dst.slot, src.reg);
			}
		} else {
			if (src.reg == NO_REG) {
				if (Trace.compiler) OUT.put3("    emitMove[%s] %s <- slot=%d", dst.kind.name, name(dst.reg), src.slot).outln();
				masm.emit_mov_r_s(dst.kind, dst.reg, src.slot);
			} else {
				if (Trace.compiler) OUT.put3("    emitMove[%s] %s <- %s", dst.kind.name, name(dst.reg), name(src.reg)).outln();
				masm.emit_mov_r_r(dst.reg, src.reg);
			}
		}
	}
	def name(r: Reg) -> string {
		return masm.regAlloc.regConfig.regSet.getName(r);
	}
}

// Nodes used in the internal move graph, which can be either a register or a value slot.
// Due to the nature of parallel moves, each node can be the destination of at most one
// move. Thus the {kind} of the node actually represents the kind of destination.
def GRAY    = '\x01';
def BLACK   = '\x02';
class MoveNode {
	var slot: u32;		// slot, if reg == REG_NONE
	var reg: Reg;		// register, REG_NONE if slot
	var kind: ValueKind;	// value kind of destination
	var color: byte;	// used in traversing the graph
	var src: MoveNode;	// source of the value for this node
	var dstList: MoveNode;	// head of destination list
	var dstNext: MoveNode;	// next in a list of successors
}

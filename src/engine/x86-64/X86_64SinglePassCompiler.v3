// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// XXX: reduce duplication with MacroAssembler
def G = X86_64Regs2.toGpr, X = X86_64Regs2.toXmmr;
def R: X86_64Regs;
def C: X86_64Conds;
def A(ma: MasmAddr) -> X86_64Addr {
	return X86_64Addr.new(G(ma.base), null, 1, ma.offset);
}

// Shorten constants inside this file.
def NO_REG = SpcConsts.NO_REG;
def IS_STORED = SpcConsts.IS_STORED;
def IS_CONST = SpcConsts.IS_CONST;
def IN_REG = SpcConsts.IN_REG;
def TAG_STORED = SpcConsts.TAG_STORED;
def KIND_MASK = SpcConsts.KIND_MASK;
def KIND_I32 = SpcConsts.KIND_I32;
def KIND_I64 = SpcConsts.KIND_I64;
def KIND_F32 = SpcConsts.KIND_F32;
def KIND_F64 = SpcConsts.KIND_F64;
def KIND_V128 = SpcConsts.KIND_V128;
def KIND_REF = SpcConsts.KIND_REF;
def KIND_ABS = SpcConsts.KIND_ABS;

// Implements the target-specific parts of the single-pass compiler for X86-64.
class X86_64SinglePassCompiler extends SinglePassCompiler {
	def w = DataWriter.new();
	def mmasm = X86_64MacroAssembler.new(w, X86_64Regs2.SPC_ALLOC.copy());
	def asm = mmasm.asm;

	new(extensions: Extension.set, limits: Limits, config: RegConfig, err: ErrorGen)
		super(mmasm, extensions, limits, err) {
	}

	private def visitCompareI(asm: X86_64Assembler, cond: X86_64Cond) -> bool {
		var b = state.pop(), a = popReg();
		if (b.isConst()) asm.cmp_r_i(G(a.reg), b.const);
		else if (b.inReg()) asm.cmp_r_r(G(a.reg), G(b.reg));
		else asm.cmp_r_m(G(a.reg), A(masm.slotAddr(state.sp + 1)));
		freeVal(b);
		freeVal(a);
		var d = allocRegTos(ValueKind.I32), r1 = G(d);
		asm.set_r(cond, r1);
		asm.movbzx_r_r(r1, r1);
		state.push(KIND_I32 | IN_REG, d, 0);
		return true;
	}
	def visit_I32_EQZ() {
		state.push(KIND_I32 | IS_CONST, NO_REG, 0);
		visit_I32_EQ();
	}
	def visit_I32_EQ()   { void(tryFold_uu_z(V3Eval.I32_EQ)   || visitCompareI(asm.d, C.Z)); }
	def visit_I32_NE()   { void(tryFold_uu_z(V3Eval.I32_NE)   || visitCompareI(asm.d, C.NZ)); }
	def visit_I32_LT_S() { void(tryFold_ii_z(V3Eval.I32_LT_S) || visitCompareI(asm.d, C.L)); }
	def visit_I32_LT_U() { void(tryFold_uu_z(V3Eval.I32_LT_U) || visitCompareI(asm.d, C.C)); }
	def visit_I32_GT_S() { void(tryFold_ii_z(V3Eval.I32_GT_S) || visitCompareI(asm.d, C.G)); }
	def visit_I32_GT_U() { void(tryFold_uu_z(V3Eval.I32_GT_U) || visitCompareI(asm.d, C.A)); }
	def visit_I32_LE_S() { void(tryFold_ii_z(V3Eval.I32_LE_S) || visitCompareI(asm.d, C.LE)); }
	def visit_I32_LE_U() { void(tryFold_uu_z(V3Eval.I32_LE_U) || visitCompareI(asm.d, C.NA)); }
	def visit_I32_GE_S() { void(tryFold_ii_z(V3Eval.I32_GE_S) || visitCompareI(asm.d, C.GE)); }
	def visit_I32_GE_U() { void(tryFold_uu_z(V3Eval.I32_GE_U) || visitCompareI(asm.d, C.NC)); }

	def visit_I64_EQZ() {
		state.push(KIND_I64 | IS_CONST, NO_REG, 0);
		visit_I64_EQ();
	}
	def visit_I64_EQ()   { void(tryFold_qq_z(V3Eval.I64_EQ)   || visitCompareI(asm.q, C.Z)); }
	def visit_I64_NE()   { void(tryFold_qq_z(V3Eval.I64_NE)   || visitCompareI(asm.q, C.NZ)); }
	def visit_I64_LT_S() { void(tryFold_ll_z(V3Eval.I64_LT_S) || visitCompareI(asm.q, C.L)); }
	def visit_I64_LT_U() { void(tryFold_qq_z(V3Eval.I64_LT_U) || visitCompareI(asm.q, C.C)); }
	def visit_I64_GT_S() { void(tryFold_ll_z(V3Eval.I64_GT_S) || visitCompareI(asm.q, C.G)); }
	def visit_I64_GT_U() { void(tryFold_qq_z(V3Eval.I64_GT_U) || visitCompareI(asm.q, C.A)); }
	def visit_I64_LE_S() { void(tryFold_ll_z(V3Eval.I64_LE_S) || visitCompareI(asm.q, C.LE)); }
	def visit_I64_LE_U() { void(tryFold_qq_z(V3Eval.I64_LE_U) || visitCompareI(asm.q, C.NA)); }
	def visit_I64_GE_S() { void(tryFold_ll_z(V3Eval.I64_GE_S) || visitCompareI(asm.q, C.GE)); }
	def visit_I64_GE_U() { void(tryFold_qq_z(V3Eval.I64_GE_U) || visitCompareI(asm.q, C.NC)); }

	private def visitFloatCmp(emit_cmp: (X86_64Xmmr, X86_64Xmmr) -> X86_64Assembler, unordered_true: bool, cond: X86_64Cond) {
		var b = popReg(), a = popReg();
		var ret_zero = X86_64Label.new(), ret_one = X86_64Label.new(), done = X86_64Label.new();
		emit_cmp(X(a.reg), X(b.reg));
		asm.jc_rel_near(C.P, if(unordered_true, ret_one, ret_zero));
		asm.jc_rel_near(cond, ret_zero);
		var d = allocRegTos(ValueKind.I32);
		asm.bind(ret_one);
		asm.movd_r_i(G(d), 1);
		asm.jmp_rel_near(done);
		asm.bind(ret_zero);
		asm.movd_r_i(G(d), 0);
		asm.bind(done);
		freeVal(a);
		freeVal(b);
		state.push(KIND_I32 | IN_REG, d, 0);
	}
	def visit_F32_EQ() { visitFloatCmp(asm.ucomiss_s_s, false, C.NZ); }
	def visit_F32_NE() { visitFloatCmp(asm.ucomiss_s_s, true, C.Z); }
	def visit_F32_LT() { visitFloatCmp(asm.ucomiss_s_s, false, C.NC); }
	def visit_F32_GT() { visitFloatCmp(asm.ucomiss_s_s, false, C.NA); }
	def visit_F32_LE() { visitFloatCmp(asm.ucomiss_s_s, false, C.A); }
	def visit_F32_GE() { visitFloatCmp(asm.ucomiss_s_s, false, C.C); }

	def visit_F64_EQ() { visitFloatCmp(asm.ucomisd_s_s, false, C.NZ); }
	def visit_F64_NE() { visitFloatCmp(asm.ucomisd_s_s, true, C.Z); }
	def visit_F64_LT() { visitFloatCmp(asm.ucomisd_s_s, false, C.NC); }
	def visit_F64_GT() { visitFloatCmp(asm.ucomisd_s_s, false, C.NA); }
	def visit_F64_LE() { visitFloatCmp(asm.ucomisd_s_s, false, C.A); }
	def visit_F64_GE() { visitFloatCmp(asm.ucomisd_s_s, false, C.C); }

	def visit_REF_IS_NULL() {
		state.push(KIND_REF | IS_CONST, NO_REG, 0);
		visit_REF_EQ();
	}
	def visit_I32_CLZ() {
		void(tryFold_u_u(V3Eval.I32_CLZ)
			|| do_op1_r_r(ValueKind.I32, mmasm.emit_i32_clz_r_r));
	}
	def visit_I32_CTZ() {
		void(tryFold_u_u(V3Eval.I32_CTZ)
			|| do_op1_r_r(ValueKind.I32, mmasm.emit_i32_ctz_r_r));
	}
	def visit_I32_POPCNT() {
		void(tryFold_u_u(V3Eval.I32_POPCNT)
			|| do_op1_r_r(ValueKind.I32, asm.d.popcnt_r_r));
	}

	private def visitSimpleOp2_uu_u(fold: (u32, u32) -> u32,
		emit_r_i: (X86_64Gpr, int) -> X86_64Assembler,
		emit_r_m: (X86_64Gpr, X86_64Addr) -> X86_64Assembler,
		emit_r_r: (X86_64Gpr, X86_64Gpr) -> X86_64Assembler) {
		void(tryFold_uu_u(fold)
			|| do_op2_r_i(ValueKind.I32, emit_r_i)
			|| do_op2_r_m(ValueKind.I32, emit_r_m)
			|| do_op2_r_r(ValueKind.I32, emit_r_r));
	}
	def visit_I32_ADD() { visitSimpleOp2_uu_u(V3Eval.I32_ADD, asm.d.add_r_i, asm.d.add_r_m, asm.d.add_r_r); }
	def visit_I32_SUB() { visitSimpleOp2_uu_u(V3Eval.I32_SUB, asm.d.sub_r_i, asm.d.sub_r_m, asm.d.sub_r_r); }
	def visit_I32_MUL() { visitSimpleOp2_uu_u(V3Eval.I32_MUL, asm.d.imul_r_i, asm.d.imul_r_m, asm.d.imul_r_r); }

	private def visitIDivRem(dst: Reg, emit: X86_64Gpr -> void) {
		var b = popFixedReg(X86_64Regs2.RCX); // XXX: use any reg except RAX or RDX
		var a = popFixedReg(X86_64Regs2.RAX);
		spillReg(X86_64Regs2.RDX);
		emit(G(b.reg));
		freeVal(b);
		freeVal(a);
		masm.regAlloc.unfree(dst, int.!(state.sp));
		state.push(a.kindFlagsAndTag(IN_REG), dst, 0);
	}
	def visit_I32_DIV_S() { visitIDivRem(X86_64Regs2.RAX, mmasm.emit_i32_div_s); } // XXX: fold potentially-trapping div/rem
	def visit_I32_DIV_U() { visitIDivRem(X86_64Regs2.RAX, mmasm.emit_i32_div_u); }
	def visit_I32_REM_S() { visitIDivRem(X86_64Regs2.RDX, mmasm.emit_i32_rem_s); }
	def visit_I32_REM_U() { visitIDivRem(X86_64Regs2.RDX, mmasm.emit_i32_rem_u); }

	private def visitShift(kind: ValueKind, emit_r_i: (X86_64Gpr, u6) -> X86_64Assembler, emit_r_cl: X86_64Gpr -> X86_64Assembler) -> bool {
		var b = state.peek(), a: SpcVal;
		if (b.isConst()) {
			state.pop();
			a = popRegToOverwrite();
			emit_r_i(G(a.reg), u6.view(b.const));
		} else {
			b = popFixedReg(X86_64Regs2.RCX);
			a = popRegToOverwrite();
			emit_r_cl(G(a.reg));
		}
		freeVal(b);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, a.reg, 0); // XXX: tag preservation
		return true;
	}
	def visit_I32_SHL()   { void(tryFold_ii_i(V3Eval.I32_SHL) || visitShift(ValueKind.I32, asm.d.shl_r_i, asm.d.shl_r_cl)); }
	def visit_I32_SHR_S() { void(tryFold_ii_i(V3Eval.I32_SHR_S) || visitShift(ValueKind.I32, asm.d.sar_r_i, asm.d.sar_r_cl)); }
	def visit_I32_SHR_U() { void(tryFold_ii_i(V3Eval.I32_SHR_U) || visitShift(ValueKind.I32, asm.d.shr_r_i, asm.d.shr_r_cl)); }
	def visit_I32_ROTL()  { void(tryFold_uu_u(V3Eval.I32_ROTL) || visitShift(ValueKind.I32, asm.d.rol_r_i, asm.d.rol_r_cl)); }
	def visit_I32_ROTR()  { void(tryFold_uu_u(V3Eval.I32_ROTR) || visitShift(ValueKind.I32, asm.d.ror_r_i, asm.d.ror_r_cl)); }
	def visit_I32_AND() { visitSimpleOp2_uu_u(V3Eval.I32_AND, asm.d.and_r_i, asm.d.and_r_m, asm.d.and_r_r); }
	def visit_I32_OR()  { visitSimpleOp2_uu_u(V3Eval.I32_OR, asm.d.or_r_i, asm.d.or_r_m, asm.d.or_r_r); }
	def visit_I32_XOR() { visitSimpleOp2_uu_u(V3Eval.I32_XOR, asm.d.xor_r_i, asm.d.xor_r_m, asm.d.xor_r_r); }

	def visit_I64_CLZ() {
		void(tryFold_q_q(V3Eval.I64_CLZ)
			|| do_op1_r_r(ValueKind.I64, mmasm.emit_i64_clz_r_r));
	}
	def visit_I64_CTZ() {
		void(tryFold_q_q(V3Eval.I64_CTZ)
			|| do_op1_r_r(ValueKind.I64, mmasm.emit_i64_ctz_r_r));
	}
	def visit_I64_POPCNT() {
		void(tryFold_q_q(V3Eval.I64_POPCNT)
			|| do_op1_r_r(ValueKind.I64, asm.q.popcnt_r_r));
	}
	private def visitSimpleOp2_qq_q(fold: (u64, u64) -> u64,
		emit_r_i: (X86_64Gpr, int) -> X86_64Assembler,
		emit_r_m: (X86_64Gpr, X86_64Addr) -> X86_64Assembler,
		emit_r_r: (X86_64Gpr, X86_64Gpr) -> X86_64Assembler) {
		void(tryFold_qq_q(fold)
			|| do_op2_r_i(ValueKind.I64, emit_r_i)
			|| do_op2_r_m(ValueKind.I64, emit_r_m)
			|| do_op2_r_r(ValueKind.I64, emit_r_r));
	}
	def visit_I64_ADD() { visitSimpleOp2_qq_q(V3Eval.I64_ADD, asm.q.add_r_i, asm.q.add_r_m, asm.q.add_r_r); }
	def visit_I64_SUB() { visitSimpleOp2_qq_q(V3Eval.I64_SUB, asm.q.sub_r_i, asm.q.sub_r_m, asm.q.sub_r_r); }
	def visit_I64_MUL() { visitSimpleOp2_qq_q(V3Eval.I64_MUL, asm.q.imul_r_i, asm.q.imul_r_m, asm.q.imul_r_r); }
	def visit_I64_DIV_S() { visitIDivRem(X86_64Regs2.RAX, mmasm.emit_i64_div_s); }
	def visit_I64_DIV_U() { visitIDivRem(X86_64Regs2.RAX, mmasm.emit_i64_div_u); }
	def visit_I64_REM_S() { visitIDivRem(X86_64Regs2.RDX, mmasm.emit_i64_rem_s); }
	def visit_I64_REM_U() { visitIDivRem(X86_64Regs2.RDX, mmasm.emit_i64_rem_u); }
	def visit_I64_SHL()   { void(tryFold_qq_q(V3Eval.I64_SHL) || visitShift(ValueKind.I64, asm.q.shl_r_i, asm.q.shl_r_cl)); }
	def visit_I64_SHR_S() { void(tryFold_ll_l(V3Eval.I64_SHR_S) || visitShift(ValueKind.I64, asm.q.sar_r_i, asm.q.sar_r_cl)); }
	def visit_I64_SHR_U() { void(tryFold_qq_q(V3Eval.I64_SHR_U) || visitShift(ValueKind.I64, asm.q.shr_r_i, asm.q.shr_r_cl)); }
	def visit_I64_ROTL()  { void(tryFold_qq_q(V3Eval.I64_ROTL) || visitShift(ValueKind.I64, asm.q.rol_r_i, asm.q.rol_r_cl)); }
	def visit_I64_ROTR()  { void(tryFold_qq_q(V3Eval.I64_ROTR) || visitShift(ValueKind.I64, asm.q.ror_r_i, asm.q.ror_r_cl)); }
	def visit_I64_AND() { visitSimpleOp2_qq_q(V3Eval.I64_AND, asm.q.and_r_i, asm.q.and_r_m, asm.q.and_r_r); }
	def visit_I64_OR()  { visitSimpleOp2_qq_q(V3Eval.I64_OR, asm.q.or_r_i, asm.q.or_r_m, asm.q.or_r_r); }
	def visit_I64_XOR() { visitSimpleOp2_qq_q(V3Eval.I64_XOR, asm.q.xor_r_i, asm.q.xor_r_m, asm.q.xor_r_r); }

	// XXX: try s_m addressing mode for floating point ops
	def visit_F32_ABS() {
		var sv = popReg(), r = X(sv.reg), scratch = mmasm.scratch;
		var d = allocRegTos(ValueKind.F32);
		asm.movd_r_s(scratch, X(sv.reg));
		asm.d.and_r_i(scratch, 0x7FFFFFFF);
		asm.movd_s_r(X(d), scratch);
		state.push(sv.kindFlagsAndTag(IN_REG), d, 0);
		freeVal(sv);
	}
	def visit_F32_NEG() {
		var sv = popReg(), r = X(sv.reg), scratch = mmasm.scratch;
		var d = allocRegTos(ValueKind.F32);
		asm.movd_r_s(scratch, X(sv.reg));
		asm.d.xor_r_i(scratch, 0x80000000);
		asm.movd_s_r(X(d), scratch);
		state.push(sv.kindFlagsAndTag(IN_REG), d, 0);
		freeVal(sv);
	}
	def visit_F32_CEIL() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_POS_INF)); }
	def visit_F32_FLOOR() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_NEG_INF)); }
	def visit_F32_TRUNC() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_ZERO)); }
	def visit_F32_NEAREST() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_NEAREST)); }
	def visit_F32_SQRT() { do_op1_x_x(ValueKind.F32, asm.sqrtss_s_s); }
	def visit_F32_ADD() { do_op2_x_x(ValueKind.F32, asm.addss_s_s); }
	def visit_F32_SUB() { do_op2_x_x(ValueKind.F32, asm.subss_s_s); }
	def visit_F32_MUL() { do_op2_x_x(ValueKind.F32, asm.mulss_s_s); }
	def visit_F32_DIV() { do_op2_x_x(ValueKind.F32, asm.divss_s_s); }

	def visitFloatMinOrMax(is64: bool, isMin: bool) { // XXX: move to macro assembler?
		var b = popReg(), a = popRegToOverwrite();
		var ret_a = X86_64Label.new(), ret_b = X86_64Label.new(), is_nan = X86_64Label.new();
		var done = X86_64Label.new();
		var xmmA = X(a.reg), xmmB = X(b.reg);

		if (is64) asm.ucomisd_s_s(xmmA, xmmB);
		else asm.ucomiss_s_s(xmmA, xmmB);
		asm.jc_rel_far(C.P, is_nan);
		asm.jc_rel_near(C.C, if(isMin, ret_a, ret_b));
		asm.jc_rel_near(C.A, if(isMin, ret_b, ret_a));
		asm.movq_r_s(mmasm.scratch, xmmB); // XXX: does a 32-bit move save anything?
		if (is64) asm.q.cmp_r_i(mmasm.scratch, 0);
		else asm.d.cmp_r_i(mmasm.scratch, 0);
		asm.jc_rel_near(if(isMin, C.S, C.NS), ret_b); // handle min(-0, 0) == -0
		asm.jmp_rel_near(ret_a);

		asm.bind(is_nan);
		if (is64) {
			asm.movd_r_i(mmasm.scratch, int.view(Floats.d_nan >> 32));
			asm.q.shl_r_i(mmasm.scratch, 32);
			asm.movq_s_r(xmmA, mmasm.scratch);
		} else {
			masm.emit_mov_r_f32(a.reg, int.view(Floats.f_nan));
		}
		asm.jmp_rel_near(done);

		asm.bind(ret_b);
		if (is64) asm.movsd_s_s(xmmA, xmmB); // fallthru
		else asm.movss_s_s(xmmA, xmmB); // fallthru
		asm.bind(ret_a); // nop
		asm.bind(done);

		state.push(a.kindFlagsAndTag(IN_REG), a.reg, 0);
		freeVal(b);
	}
	def visit_F32_MIN() { visitFloatMinOrMax(false, true); }
	def visit_F32_MAX() { visitFloatMinOrMax(false, false); }
	def visit_F32_COPYSIGN() {
		var sv = popReg(), d = popRegToOverwrite(); // XXX: constant-fold and strength reduce
		var t1 = allocTmp(ValueKind.I32), t2 = allocTmp(ValueKind.I32);
		mmasm.emit_f32_copysign(X(d.reg), X(sv.reg), G(t1), G(t2));
		freeVal(sv);
		state.push(d.kindFlagsAndTag(IN_REG), d.reg, 0);
	}

	// XXX: try s_m addressing mode for floating point ops
	def visit_F64_ABS() {
		var sv = popRegToOverwrite();
		var t = allocTmp(ValueKind.I64), r1 = G(t);
		asm.movq_r_s(r1, X(sv.reg));
		asm.q.rol_r_i(r1, 1);
		asm.q.and_r_i(r1, -2);
		asm.q.ror_r_i(r1, 1);
		asm.movq_s_r(X(sv.reg), G(t));
		state.push(sv.kindFlagsAndTag(IN_REG), sv.reg, 0);
	}
	def visit_F64_NEG() {
		var sv = popRegToOverwrite();
		var t = allocTmp(ValueKind.I64), r1 = G(t);
		asm.movq_r_s(r1, X(sv.reg));
		asm.q.rol_r_i(r1, 1);
		asm.q.xor_r_i(r1, 1);
		asm.q.ror_r_i(r1, 1);
		asm.movq_s_r(X(sv.reg), G(t));
		state.push(sv.kindFlagsAndTag(IN_REG), sv.reg, 0);
	}
	def visit_F64_CEIL() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_POS_INF)); }
	def visit_F64_FLOOR() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_NEG_INF)); }
	def visit_F64_TRUNC() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_ZERO)); }
	def visit_F64_NEAREST() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_NEAREST)); }
	def visit_F64_SQRT() { do_op1_x_x(ValueKind.F64, asm.sqrtsd_s_s); }
	def visit_F64_ADD() { do_op2_x_x(ValueKind.F64, asm.addsd_s_s); }
	def visit_F64_SUB() { do_op2_x_x(ValueKind.F64, asm.subsd_s_s); }
	def visit_F64_MUL() { do_op2_x_x(ValueKind.F64, asm.mulsd_s_s); }
	def visit_F64_DIV() { do_op2_x_x(ValueKind.F64, asm.divsd_s_s); }
	def visit_F64_MIN() { visitFloatMinOrMax(true, true); }
	def visit_F64_MAX() { visitFloatMinOrMax(true, false); }
	def visit_F64_COPYSIGN() {
		var sv = popReg(), d = popRegToOverwrite(); // XXX: constant-fold and strength reduce
		var t1 = allocTmp(ValueKind.I64), t2 = allocTmp(ValueKind.I64);
		mmasm.emit_f64_copysign(X(d.reg), X(sv.reg), G(t1), G(t2));
		freeVal(sv);
		state.push(d.kindFlagsAndTag(IN_REG), d.reg, 0);
	}

	private def visitITruncF(opcode: Opcode) {
		var x1 = popReg();
		var dkind = if(opcode.sig.results[0] == ValueType.I32, ValueKind.I32, ValueKind.I64);
		var xscratch = allocTmp(if(opcode.sig.params[0] == ValueType.F32, ValueKind.F32, ValueKind.F64));
		var d = allocRegTos(dkind);
		mmasm.emit_i_trunc_f(opcode, G(d), X(x1.reg), X(xscratch));
		freeVal(x1);
		state.push(SpcConsts.kindToFlags(dkind) | IN_REG, d, 0);
	}

	def visit_I32_WRAP_I64() {
		void(tryFold_l_i(i32.view<i64>)
			|| do_op1_r_r(ValueKind.I32, asm.movd_r_r));
	}
	def visit_I32_TRUNC_F32_S() { visitITruncF(Opcode.I32_TRUNC_F32_S); }
	def visit_I32_TRUNC_F32_U() { visitITruncF(Opcode.I32_TRUNC_F32_U); }
	def visit_I32_TRUNC_F64_S() { visitITruncF(Opcode.I32_TRUNC_F64_S); }
	def visit_I32_TRUNC_F64_U() { visitITruncF(Opcode.I32_TRUNC_F64_U); }
	def visit_I64_EXTEND_I32_S() {
		void(tryFold_i_l(i64.view<i32>)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_s));
	}
	def visit_I64_TRUNC_F32_S() { visitITruncF(Opcode.I64_TRUNC_F32_S); }
	def visit_I64_TRUNC_F32_U() { visitITruncF(Opcode.I64_TRUNC_F32_U); }
	def visit_I64_TRUNC_F64_S() { visitITruncF(Opcode.I64_TRUNC_F64_S); }
	def visit_I64_TRUNC_F64_U() { visitITruncF(Opcode.I64_TRUNC_F64_U); }
	def visit_I64_EXTEND_I32_U() {
		void(tryFold_u_l(i64.view<u32>)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_u));
	}
	def visitReinterpret(kind: ValueKind) {
		var sv = state.pop(), flags = SpcConsts.kindToFlags(kind);
		if (sv.isConst()) {
			state.push(flags | IS_CONST, NO_REG, sv.const);
		} else if (sv.inReg()) {
			var d = allocRegTos(kind);
			match (kind) {
				I32 => asm.movd_r_s(G(d), X(sv.reg));
				I64 => asm.movq_r_s(G(d), X(sv.reg));
				F32 => asm.movd_s_r(X(d), G(sv.reg));
				F64 => asm.movq_s_r(X(d), G(sv.reg));
				_ => bailout("unexpected value kind");
			}
			state.push(flags | IN_REG, d, 0);
			freeVal(sv);
		} else {
			state.push(flags | IS_STORED, NO_REG, 0);
		}
	}
	def visit_I32_REINTERPRET_F32() { visitReinterpret(ValueKind.I32); }
	def visit_I64_REINTERPRET_F64() { visitReinterpret(ValueKind.I64); }
	def visit_F32_REINTERPRET_I32() { visitReinterpret(ValueKind.F32); }
	def visit_F64_REINTERPRET_I64() { visitReinterpret(ValueKind.F64); }

	def visit_I32_EXTEND8_S() {
		void(tryFold_i_i(V3Eval.I32_EXTEND8_S)
			|| do_op1_r_r(ValueKind.I32, asm.d.movbsx_r_r));
	}
	def visit_I32_EXTEND16_S() {
		void(tryFold_i_i(V3Eval.I32_EXTEND16_S)
			|| do_op1_r_r(ValueKind.I32, asm.d.movwsx_r_r));
	}
	def visit_I64_EXTEND8_S() {
		void(tryFold_l_l(V3Eval.I64_EXTEND8_S)
			|| do_op1_r_r(ValueKind.I64, asm.q.movbsx_r_r));
	}
	def visit_I64_EXTEND16_S() {
		void(tryFold_l_l(V3Eval.I64_EXTEND16_S)
			|| do_op1_r_r(ValueKind.I64, asm.q.movwsx_r_r));
	}
	def visit_I64_EXTEND32_S() {
		void(tryFold_l_l(V3Eval.I64_EXTEND32_S)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_s));
	}
	def visit_REF_EQ()   { visitCompareI(asm.q, C.Z); }

	def visit_I32_TRUNC_SAT_F32_S() { visitITruncF(Opcode.I32_TRUNC_SAT_F32_S); }
	def visit_I32_TRUNC_SAT_F32_U() { visitITruncF(Opcode.I32_TRUNC_SAT_F32_U); }
	def visit_I32_TRUNC_SAT_F64_S() { visitITruncF(Opcode.I32_TRUNC_SAT_F64_S); }
	def visit_I32_TRUNC_SAT_F64_U() { visitITruncF(Opcode.I32_TRUNC_SAT_F64_U); }
	def visit_I64_TRUNC_SAT_F32_S() { visitITruncF(Opcode.I64_TRUNC_SAT_F32_S); }
	def visit_I64_TRUNC_SAT_F32_U() { visitITruncF(Opcode.I64_TRUNC_SAT_F32_U); }
	def visit_I64_TRUNC_SAT_F64_S() { visitITruncF(Opcode.I64_TRUNC_SAT_F64_S); }
	def visit_I64_TRUNC_SAT_F64_U() { visitITruncF(Opcode.I64_TRUNC_SAT_F64_U); }

	private def visitFConvertI32S(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr) -> X86_64Assembler) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		asm.q.shl_r_i(r1, 32);
		asm.q.sar_r_i(r1, 32); // sign-extend
		emit_cvt(X(d), r1);
		freeVal(sv);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F32_CONVERT_I32_S() { visitFConvertI32S(ValueKind.F32, asm.cvtsi2ss_s_r); }
	def visit_F64_CONVERT_I32_S() { visitFConvertI32S(ValueKind.F64, asm.cvtsi2sd_s_r); }
	private def visitFConvertI32U(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr) -> X86_64Assembler) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		asm.movd_r_r(r1, r1); // zero-extend
		emit_cvt(X(d), r1);
		freeVal(sv);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F32_CONVERT_I32_U() { visitFConvertI32U(ValueKind.F32, asm.cvtsi2ss_s_r); }
	def visit_F64_CONVERT_I32_U() { visitFConvertI32U(ValueKind.F64, asm.cvtsi2sd_s_r); }
	private def visitFConvertI64S(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr) -> X86_64Assembler) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		emit_cvt(X(d), r1);
		freeVal(sv);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	private def visitFConvertI64U(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr, X86_64Xmmr, X86_64Gpr) -> void) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		var xscratch = allocTmp(kind);
		emit_cvt(X(d), r1, X(xscratch), mmasm.scratch);
		freeVal(sv);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F32_CONVERT_I64_S() { visitFConvertI64S(ValueKind.F32, asm.cvtsi2ss_s_r); }
	def visit_F32_CONVERT_I64_U() { visitFConvertI64U(ValueKind.F32, mmasm.emit_f32_convert_i64_u); }
	def visit_F64_CONVERT_I64_S() { visitFConvertI64S(ValueKind.F64, asm.cvtsi2sd_s_r); }
	def visit_F64_CONVERT_I64_U() { visitFConvertI64U(ValueKind.F64, mmasm.emit_f64_convert_i64_u); }

	def visit_F32_DEMOTE_F64() { do_op1_x_x(ValueKind.F32, asm.cvtsd2ss_s_s); } // XXX: try s_m addr mode
	def visit_F64_PROMOTE_F32() { do_op1_x_x(ValueKind.F64, asm.cvtss2sd_s_s); } // XXX: try s_m addr mode

	// r1 = op(r1)
	private def do_op1_r<T>(kind: ValueKind, emit: (X86_64Gpr -> T)) -> bool {
		var sv = popRegToOverwrite(), r = G(sv.reg);
		emit(r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), sv.reg, 0);
		return true;
	}
	// r1 = op(r2)
	private def do_op1_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr) -> T) -> bool {
		var sv = popReg(), r = G(sv.reg);
		var d = allocRegTos(kind);
		emit(G(d), r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), d, 0);
		freeVal(sv);
		return true;
	}
	// r1 = op(r1, r2)
	private def do_op2_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr) -> T) -> bool {
		var b = popReg();
		var a = popRegToOverwrite();
		emit(G(a.reg), G(b.reg));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		freeVal(b);
		return true;
	}
	// r1 = op(r1, m2)
	private def do_op2_r_m<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Addr) -> T) -> bool {
		var b = state.peek();
		if (b.inReg() || b.isConst()) return false;
		state.pop();
		var addr = masm.slotAddr(state.sp);
		var a = popRegToOverwrite();
		emit(G(a.reg), A(addr));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// r1 = op(r1, imm)
	private def do_op2_r_i<T>(kind: ValueKind, emit: (X86_64Gpr, int) -> T) -> bool {
		var b = state.peek();
		if (!b.isConst()) return false;
		state.pop();
		var a = popRegToOverwrite();
		emit(G(a.reg), b.const);
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// r1 = op(r2, imm)
	private def do_op2_r_r_i<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, int) -> T) -> bool;
	// r1 = op(r2, m3)
	private def do_op2_r_r_m<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, X86_64Addr) -> T) -> bool;
	// r1 = op(r2, r3)
	private def do_op2_r_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, X86_64Gpr) -> T) -> bool;
	// r1 = op(r2)
	private def do_op1_x_x<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var sv = popReg(), r = X(sv.reg);
		var d = allocRegTos(kind);
		emit(X(d), r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), d, 0);
		freeVal(sv);
		return true;
	}
	// x1 = op(x1, x2)
	private def do_op2_x_x<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var b = popReg();
		var a = popRegToOverwrite();
		emit(X(a.reg), X(b.reg));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		freeVal(b);
		return true;
	}
}

def ucontext_rip_offset = 168;
def ucontext_rsp_offset = 160;
def SIGFPE  = 8;
def SIGBUS  = 10;
def SIGSEGV = 11;
def globalBuf = StringBuilder.new().grow(256);  // avoid allocations when describing frames

// Represents the JITed code for an entire module.
// Implements the RiUserCode interface to add generated machine code to the V3 runtime.
// Handles stackwalking and signals in JITed code.
class X86_64SpcCode extends RiUserCode {
	def mapping: Mapping;
	def frameSize = IVarConfig.frameSize;
	def trapHandlerOffsets = Array<int>.new(TrapReason.ERROR.tag + 1);
	var codeEnd: int;			// for dynamically adding code to the end

	new(mapping) super(mapping.range.start, mapping.range.end) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: (Array<byte>, int, int) -> ()) {
		var msg = "\tin [spc-jit] ";
		out(msg, 0, msg.length);
		var instance = (sp + IVarConfig.frame.INSTANCE.disp).load<Instance>();
		var wf = (sp + IVarConfig.frame.WASM_FUNC.disp).load<WasmFunction>();
		// TODO: lazy parse of names section may allocate; must avoid this in OOM situation
		var buf = globalBuf;
		wf.decl.render(instance.module.names, buf);
		buf.ln().out(out);
		buf.reset();
	}

	// Called from V3 runtime for a frame where {ip} is in JIT code.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp += frameSize;	 // assume frame is allocated
		ip = sp.load<Pointer>(); // return address on stack
		return (ip + -1, sp + Pointer.SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}

	// Called from V3 runtime when the garbage collector needs to scan a JIT stack frame.
	def scanFrame(ip: Pointer, sp: Pointer) {
		// Handle other roots in the frame
		RiGc.scanRoot(sp + IVarConfig.frame.WASM_FUNC.disp);
		RiGc.scanRoot(sp + IVarConfig.frame.INSTANCE.disp);
		RiGc.scanRoot(sp + IVarConfig.frame.ACCESSOR.disp);
	}

	// Called from V3 runtime to handle an OS-level signal that occurred while {ip} was in JIT code.
	def handleSignal(signum: int, siginfo: Pointer, ucontext: Pointer, ip: Pointer, sp: Pointer) -> bool {
		var pip = ucontext + ucontext_rip_offset;
		var ip = pip.load<Pointer>();
		if (Trace.interpreter) {
			Trace.OUT.put2("  !signal %d in JIT code @ 0x%x", signum, ip - Pointer.NULL).outln();
		}
		match (signum) {
			SIGFPE => {
				// presume divide/modulus by zero
				pip.store<Pointer>(start + trapHandlerOffsets[TrapReason.DIV_BY_ZERO.tag]);
				return true;
			}
			SIGBUS, SIGSEGV => {
				var addr = RiOs.getAccessAddress(siginfo, ucontext);
				if (RedZones.isInRedZone(addr)) {
					pip.store<Pointer>(start + trapHandlerOffsets[TrapReason.STACK_OVERFLOW.tag]);
					return true;
				}
				pip.store<Pointer>(start + trapHandlerOffsets[TrapReason.MEM_OUT_OF_BOUNDS.tag]);
				return true;
			}
		}
		return true;
	}
	def keepAlive() { // XXX: need to trick V3 whole-program optimizer to not delete these fields
		if (mapping == null) System.error(null, null);
		if (mapping.range == null) System.error(null, null);
	}
	// Get a frame accessor for the probe API.
	def getFrameAccessor(sp: Pointer) -> X86_64SpcFrameAccessor {
		var retip = (sp + -Pointer.SIZE).load<Pointer>();
		if (!mapping.range.contains(retip)) return null;
		if (true) { // XXX: tuning for caching frame accessor object
			var prev = (sp + IVarConfig.frame.ACCESSOR.disp).load<X86_64SpcFrameAccessor>();
			if (prev != null) return prev;
			var n = X86_64SpcFrameAccessor.new(sp);
			(sp + IVarConfig.frame.ACCESSOR.disp).store<X86_64SpcFrameAccessor>(n);
			return n;
		}
		return X86_64SpcFrameAccessor.new(sp);
	}
	def installCodeFor(m: Module, func_index: int, masm: X86_64MacroAssembler) {
	}
}

// The lazy-compile stub needs special handling in the Virgil runtime because it has
// a frame that stores the function being compiled.
class X86_64SpcCompileStub extends RiUserCode {
	def stubName: string;
	def frameSize = Pointer.SIZE;
	
	new(stubName, start: Pointer, end: Pointer) super(start, end) { }
	
	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: (Array<byte>, int, int) -> ()) {
		var msg = "\tin [spc-";
		out(msg, 0, msg.length);
		out(stubName, 0, stubName.length);
		msg = "-compile-stub] ";
		out(msg, 0, msg.length);
		var wf = (sp + 0).load<WasmFunction>();
		// TODO: lazy parse of names section may allocate; must avoid this in OOM situation
		var buf = globalBuf;
		wf.decl.render(wf.instance.module.names, buf);
		buf.ln().out(out);
		buf.reset();
	}
	// Called from V3 runtime for a frame where {ip} is in the stub code.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp += frameSize;	 // assume frame is allocated
		ip = sp.load<Pointer>(); // return address on stack
		return (ip + -1, sp + Pointer.SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}
	// Called from V3 runtime when the garbage collector needs to scan a JIT stack frame.
	def scanFrame(ip: Pointer, sp: Pointer) {
		// The WasmFunction is stored in the frame for debugging.
		RiGc.scanRoot(sp + 0);
	}
}

// Implements frame accessor for JIT compiled method frames. For SPC, the frame layout is almost
// identical to interpreter frames, so it inherits from the base implementation.
class X86_64SpcFrameAccessor extends X86_64BaseFrameAccessor {
	new(sp: Pointer) super(sp) {
		var wf = (sp + IVarConfig.frame.WASM_FUNC.disp).load<WasmFunction>();
		decl = wf.decl;
	}

	// Returns {true} if this frame has been unwound, either due to returning, a trap, or exception.
	def isUnwound() -> bool {
		return this != (sp + IVarConfig.frame.ACCESSOR.disp).load<FrameAccessor>();
	}
	// Returns the current program counter.
	def pc() -> int {
		return 0; // TODO: compute PC from return IP of callee
	}
	// Set the value of a local variable. (dynamically typechecked).
	def setLocal(i: int, v: Value); // TODO: translate to interpreter frame
	// Set operand at depth {i}, with 0 being the top of the stack, -1 being one lower, etc. (dynamically typechecked).
	def setOperand(i: int, v: Value); // TODO: translate to interpreter frame
}

// Support for V3 -> SPC calls.
var entryStub: (WasmFunction, Pointer, Pointer) -> AbruptReturn;
var lazyCompileStub: Pointer;
var tierUpCompileStub: Pointer;
var stubCode: Mapping;
// An unpacked closure for lazy compilation.
var lazyCompileFunc: Pointer;
var tierUpCompileFunc: Pointer;
var lazyCompileObject: ExecutionStrategy;

// Global functionality associated with the single-pass compiler for X86-64.
component X86_64Spc {
	// Generate and map code for global stubs.
	def genStubs() {
		if (entryStub != null) return;

		// =========== SPC entry stub ================================
		var w = DataWriter.new();
		var masm = X86_64MacroAssembler.new(w, X86_64Regs2.SPC_ALLOC.copy());
		var asm = X86_64Assembler.!(masm.asm);
		var c = X86_64Regs2.CONFIG;
		var func_arg = G(c.regs.func_arg);
		if (func_arg == Target.V3_PARAM_GPRS[2]) {
			// XXX: use macro assembler to order moves
			var scratch = R.R15; 					// TODO: use scratch from macro asm
			asm.movq_r_r(scratch, func_arg);
			asm.movq_r_r(func_arg, Target.V3_PARAM_GPRS[1]);	// function
			asm.movq_r_r(G(c.regs.vsp), scratch);			// vsp
		} else {
			asm.movq_r_r(func_arg, Target.V3_PARAM_GPRS[1]);	// function
			asm.movq_r_r(G(c.regs.vsp), Target.V3_PARAM_GPRS[2]);	// vsp
		}
		asm.ijmp_r(Target.V3_PARAM_GPRS[3]);				// tail-call entrypoint
		asm.invalid();

		// =========== SPC lazy compile stub ========================
		var lazy_offset = w.pos; {
			asm.pushq_r(G(c.regs.func_arg));					// push function onto stack
			masm.emit_store_runtime_vsp(c.regs.vsp);
			asm.movq_r_r(Target.V3_PARAM_GPRS[1], G(c.regs.func_arg));	// function
			// Load {lazyCompileObject}.
			var obj_ptr = int.view(u32.!((Pointer.atField(lazyCompileObject) - Pointer.NULL)));
			asm.movq_r_m(Target.V3_PARAM_GPRS[0], X86_64Addr.new(null, null, 1, obj_ptr));
			// Call the {lazyCompileFunc}.
			var func_ptr = int.view(u32.!(Pointer.atField(lazyCompileFunc) - Pointer.NULL));
			asm.icall_m(X86_64Addr.new(null, null, 1, func_ptr));
			asm.q.add_r_i(R.RSP, Pointer.SIZE);				// pop function off stack
			// Check for non-null abrupt return.
			var unwind = X86_64Label.new();
			asm.q.cmp_r_i(Target.V3_RET_GPRS[2], 0);
			asm.jc_rel_near(C.NZ, unwind);
			// Tail-call the result of the compile.
			var scratch = X86_64Regs.R15;
			asm.movq_r_r(scratch, Target.V3_RET_GPRS[1]);			// entrypoint
			asm.movq_r_r(G(c.regs.func_arg), Target.V3_RET_GPRS[0]);	// function
			masm.emit_load_runtime_vsp(c.regs.vsp);
			asm.ijmp_r(scratch);						// jump to entrypoint
			asm.invalid();
			// Simply return the {AbruptReturn} object.
			asm.bind(unwind);
			asm.movq_r_r(Target.V3_RET_GPRS[0], Target.V3_RET_GPRS[2]);
			asm.ret();
		}
		var lazy_end = w.pos;

		// =========== SPC tier-up compile stub ========================
		var tierup_offset = w.pos; {
			// Decrement execution counter by 1 and compile if <= 0
			var scratch = R.RBP;
			var offsets = V3Offsets.new();
			asm.movq_r_m(scratch, G(c.regs.func_arg).plus(offsets.WasmFunction_decl));
			asm.q.sub_m_i(scratch.plus(offsets.FuncDecl_tierup_trigger), 1);
			asm.jc_rel_near(C.NA, interpreter_entry);

			asm.pushq_r(G(c.regs.func_arg));				// push function onto stack
			masm.emit_store_runtime_vsp(c.regs.vsp);
			asm.movq_r_r(Target.V3_PARAM_GPRS[1], G(c.regs.func_arg));	// function
			// Load {lazyCompileObject}.
			var obj_ptr = int.view(u32.!((Pointer.atField(lazyCompileObject) - Pointer.NULL)));
			asm.movq_r_m(Target.V3_PARAM_GPRS[0], X86_64Addr.new(null, null, 1, obj_ptr));
			// Call the {tierUpCompileFunc}.
			var func_ptr = int.view(u32.!(Pointer.atField(tierUpCompileFunc) - Pointer.NULL));
			asm.icall_m(X86_64Addr.new(null, null, 1, func_ptr));
			asm.q.add_r_i(R.RSP, Pointer.SIZE);				// pop function off stack
			// Tail-call the result of the compile (which could be interpreter entry).
			asm.movq_r_r(scratch, Target.V3_RET_GPRS[1]);			// entrypoint
			asm.movq_r_r(G(c.regs.func_arg), Target.V3_RET_GPRS[0]);	// function
			masm.emit_load_runtime_vsp(c.regs.vsp);
			asm.ijmp_r(scratch);						// jump to entrypoint
			asm.invalid();
		}
		var tierup_end = w.pos;

		stubCode = Target.mapCode(asm, null);
		entryStub = CiRuntime.forgeClosure<void, (WasmFunction, Pointer, Pointer), AbruptReturn>(stubCode.range.start, ());
		lazyCompileStub = stubCode.range.start + lazy_offset;
		// Register user code to allow walking over a lazy compile frame.
		RiRuntime.registerUserCode(X86_64SpcCompileStub.new("lazy",
			lazyCompileStub, stubCode.range.start + lazy_end));

		tierUpCompileStub = stubCode.range.start + tierup_offset;
		// Register user code to allow walking over a lazy compile frame.
		RiRuntime.registerUserCode(X86_64SpcCompileStub.new("tier-up",
			tierUpCompileStub, stubCode.range.start + tierup_end));

		if (Trace.compiler) Trace.OUT.put1("spc_entry: break *0x%x", stubCode.range.start - Pointer.NULL).outln();
		if (Trace.compiler) Trace.OUT.put1("spc_lazy_compile: break *0x%x", lazyCompileStub - Pointer.NULL).outln();
		if (Trace.compiler) Trace.OUT.put1("spc_tier_up_compile: break *0x%x", tierUpCompileStub - Pointer.NULL).outln();
	}
	def setLazyCompile(lazyCompile: WasmFunction -> (WasmFunction, Pointer, AbruptReturn)) {
		if (lazyCompile == null) {
			lazyCompileObject = null;
			lazyCompileFunc = Pointer.NULL;
		} else {
			var t = CiRuntime.unpackClosure<ExecutionStrategy, WasmFunction, (WasmFunction, Pointer, AbruptReturn)>(lazyCompile);
			lazyCompileFunc = t.0;
			lazyCompileObject = t.1;
		}
	}
	def setTierUpCompile(tierUpCompile: WasmFunction -> (WasmFunction, Pointer, AbruptReturn)) {
		if (tierUpCompile == null) {
			lazyCompileObject = null;
			tierUpCompileFunc = Pointer.NULL;
		} else {
			var t = CiRuntime.unpackClosure<ExecutionStrategy, WasmFunction, (WasmFunction, Pointer, AbruptReturn)>(tierUpCompile);
			tierUpCompileFunc = t.0;
			lazyCompileObject = t.1;
		}
	}
	// A handy chokepoint for entering JIT code from V3.
	def invoke(wf: WasmFunction, sp: Pointer) -> AbruptReturn {
		return entryStub(wf, sp, wf.decl.target_code.spc_entry);
	}
	def setLazyCompileFor(decl: FuncDecl) {
		decl.target_code = TargetCode(lazyCompileStub);
	}
	def setTierUpFor(decl: FuncDecl) {
		decl.target_code = TargetCode(tierUpCompileStub);
	}
	def returnCompileFailed(wf: WasmFunction, err: ErrorGen) -> (WasmFunction, Pointer, AbruptReturn) {
		var buf = StringBuilder.new();
		buf.puts("compile[");
		wf.decl.render(wf.instance.module.names, buf);
		buf.puts("]: ");
		buf.puts(err.error_msg);
		var msg = buf.toString();
		if (Trace.compiler) Trace.OUT.puts(msg).outln();
		return (null, Pointer.NULL, InternalError.new(msg, null)); // XXX: gather stacktrace?
	}
	def estimateCodeSizeFor(decl: FuncDecl) -> int {
		return 60 + decl.orig_bytecode.length * 20; // TODO: huge overestimate
	}
}
